{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-21T06:31:17.718597Z","iopub.execute_input":"2021-05-21T06:31:17.719125Z","iopub.status.idle":"2021-05-21T06:31:17.736066Z","shell.execute_reply.started":"2021-05-21T06:31:17.719092Z","shell.execute_reply":"2021-05-21T06:31:17.734566Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/input/aaamlp/adult.csv\n/kaggle/input/aaamlp/mnist_test.csv\n/kaggle/input/aaamlp/mobile_train.csv\n/kaggle/input/aaamlp/cat_test.csv\n/kaggle/input/aaamlp/cat_train_folds.csv\n/kaggle/input/aaamlp/mnist_train_folds.csv\n/kaggle/input/aaamlp/cat_train.csv\n/kaggle/input/aaamlp/mnist_train.csv\n/kaggle/input/aaamlp/winequality-red.csv\n/kaggle/input/aaamlp/imdb_folds.csv\n/kaggle/input/aaamlp/adult_folds.csv\n/kaggle/input/aaamlp/imdb.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import datasets\nfrom sklearn import manifold\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = datasets.fetch_openml(\n'mnist_784',\nversion=1,\nreturn_X_y=True\n)\npixel_values, targets = data\ntargets = targets.astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pixel_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"single_image = pixel_values.iloc[5, :].values.reshape(28, 28)\nplt.imshow(single_image, cmap='gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne = manifold.TSNE(n_components=2, random_state=42)\ntransformed_data = tsne.fit_transform(pixel_values.iloc[:3000, :])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne_df = pd.DataFrame(\nnp.column_stack((transformed_data, targets[:3000])),\ncolumns=[\"x\", \"y\", \"targets\"]\n)\ntsne_df.loc[:, \"targets\"] = tsne_df.targets.astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid = sns.FacetGrid(tsne_df, hue=\"targets\", size=8)\ngrid.map(plt.scatter, \"x\", \"y\").add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/aaamlp/winequality-red.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a mapping dictionary that maps the quality values from 0 to 5\nquality_mapping = {\n3: 0,\n4: 1,\n5: 2,\n6: 3,\n7: 4,\n8: 5\n}\n# you can use the map function of pandas with\n# any dictionary to convert the values in a given\n# column to values in the dictionary\ndf.loc[:, \"quality\"] = df.quality.map(quality_mapping)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use sample with frac=1 to shuffle the dataframe\n# we reset the indices since they change after\n# shuffling the dataframe\ndf = df.sample(frac=1).reset_index(drop=True)\n# top 1000 rows are selected\n# for training\ndf_train = df.head(1000)\n# bottom 599 values are selected\n# for testing/validation\ndf_test = df.tail(599)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import from scikit-learn\nfrom sklearn import tree\nfrom sklearn import metrics\n# initialize decision tree classifier class\n# with a max_depth of 3\nclf = tree.DecisionTreeClassifier(max_depth=3)\n# choose the columns you want to train on\n# these are the features for the model\ncols = ['fixed acidity',\n'volatile acidity',\n'citric acid',\n'residual sugar',\n'chlorides',\n'free sulfur dioxide',\n'total sulfur dioxide',\n'density',\n'pH',\n'sulphates',\n'alcohol']\n# train the model on the provided features\n# and mapped quality from before\nclf.fit(df_train[cols], df_train.quality)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate predictions on the training set\ntrain_predictions = clf.predict(df_train[cols])\n# generate predictions on the test set\ntest_predictions = clf.predict(df_test[cols])\n# calculate the accuracy of predictions on\n# training data set\ntrain_accuracy = metrics.accuracy_score(\ndf_train.quality, train_predictions\n)\n# calculate the accuracy of predictions on\n# test data set\ntest_accuracy = metrics.accuracy_score(\ndf_test.quality, test_predictions\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_accuracy, test_accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import scikit-learn tree and metrics\nfrom sklearn import tree\nfrom sklearn import metrics\n# import matplotlib and seaborn\n# for plotting\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# this is our global size of label text\n# on the plots\nmatplotlib.rc('xtick', labelsize=20)\nmatplotlib.rc('ytick', labelsize=20)\n# This line ensures that the plot is displayed\n# inside the notebook\n%matplotlib inline\n# initialize lists to store accuracies\n# for training and test data\n# we start with 50% accuracy\ntrain_accuracies = [0.5]\ntest_accuracies = [0.5]\n# iterate over a few depth values\nfor depth in range(1, 25):\n# init the model\n    clf = tree.DecisionTreeClassifier(max_depth=depth)\n# columns/features for training\n# note that, this can be done outside\n# the loop\n    cols = [\n    'fixed acidity',\n    'volatile acidity',\n    'citric acid',\n    'residual sugar',\n    'chlorides',\n    'free sulfur dioxide',\n    'total sulfur dioxide',\n    'density',\n    'pH',\n    'sulphates',\n    'alcohol'\n]\n# fit the model on given features\n    clf.fit(df_train[cols], df_train.quality)\n# create training & test predictions\n    train_predictions = clf.predict(df_train[cols])\n    test_predictions = clf.predict(df_test[cols])\n# calculate training & test accuracies\n    train_accuracy = metrics.accuracy_score(\n    df_train.quality, train_predictions\n    )\n    test_accuracy = metrics.accuracy_score(\n    df_test.quality, test_predictions\n    )\n# append accuracies\n    train_accuracies.append(train_accuracy)\n    test_accuracies.append(test_accuracy)\n# create two plots using matplotlib\n# and seaborn\nplt.figure(figsize=(10, 5))\nsns.set_style(\"whitegrid\")\nplt.plot(train_accuracies, label=\"train accuracy\")\nplt.plot(test_accuracies, label=\"test accuracy\")\nplt.legend(loc=\"upper left\", prop={'size': 15})\nplt.xticks(range(0, 26, 5))\nplt.xlabel(\"max_depth\", size=20)\nplt.ylabel(\"accuracy\", size=20)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas and model_selection module of scikit-learn\nimport pandas as pd\nfrom sklearn import model_selection\nif __name__ == \"__main__\":\n# Training data is in a CSV file called train.csv\n    df = pd.read_csv(\"/kaggle/input/aaamlp/winequality-red.csv\")\n# we create a new column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n# the next step is to randomize the rows of the data\n    df = df.sample(frac=1).reset_index(drop=True)\n# initiate the kfold class from model_selection module\n    kf = model_selection.KFold(n_splits=5)\n# fill the new kfold column\n    for fold, (trn_, val_) in enumerate(kf.split(X=df)):\n        df.loc[val_, 'kfold'] = fold\n# save the new csv with kfold column\n    df.to_csv(\"train_folds.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas and model_selection module of scikit-learn\nimport pandas as pd\nfrom sklearn import model_selection\nif __name__ == \"__main__\":\n# Training data is in a csv file called train.csv\n    df = pd.read_csv(\"/kaggle/input/aaamlp/winequality-red.csv\")\n# we create a new column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n# the next step is to randomize the rows of the data\n    df = df.sample(frac=1).reset_index(drop=True)\n# fetch targets\n    y = df.quality.values\n# initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=5)\n# fill the new kfold column\n    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n        df.loc[v_, 'kfold'] = f\n# save the new csv with kfold column\n    df.to_csv(\"train_folds.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = sns.countplot(x='quality', data=df)\nb.set_xlabel(\"quality\", fontsize=20)\nb.set_ylabel(\"count\", fontsize=20)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stratified-kfold for regression\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn import model_selection\ndef create_folds(data):\n# we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n# the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n# calculate the number of bins by Sturge's rule\n# I take the floor of the value, you can also\n# just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n# bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n# initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=5)\n# fill the new kfold column\n# note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n# drop the bins column\n    data = data.drop(\"bins\", axis=1)\n# return dataframe with folds\n    return data\nif __name__ == \"__main__\":\n# we create a sample dataset with 15000 samples\n# and 100 features and 1 target\n    X, y = datasets.make_regression(\n    n_samples=15000, n_features=100, n_targets=1\n    )\n# create a dataframe out of our numpy arrays\n    df = pd.DataFrame(\n    X,\n    columns=[f\"f_{i}\" for i in range(X.shape[1])]\n    )\n    df.loc[:, \"target\"] = y\n# create folds\n    df = create_folds(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(y_true, y_pred):\n    \"\"\"\n    Function to calculate accuracy\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: accuracy score\n    \"\"\"\n# initialize a simple counter for correct predictions\n    correct_counter = 0\n# loop over all elements of y_true\n# and y_pred \"together\"\n    for yt, yp in zip(y_true, y_pred):\n        if yt == yp:\n# if prediction is equal to truth, increase the counter\n            correct_counter += 1\n# return accuracy\n# which is correct predictions over the number of samples\n    return correct_counter / len(y_true)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nl1 = [0,1,1,1,0,0,0,1]\nl2 = [0,1,0,1,0,1,0,0]\nmetrics.accuracy_score(l1, l2), accuracy(l1, l2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def true_positive(y_true, y_pred):\n    \"\"\"\n    Function to calculate True Positives\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: number of true positives\n    \"\"\"\n    # initialize\n    tp = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 1 and yp == 1:\n            tp += 1\n    return tp\ndef true_negative(y_true, y_pred):\n    \"\"\"\n    Function to calculate True Negatives\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: number of true negatives\n    \"\"\"\n    # initialize\n    tn = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 0 and yp == 0:\n            tn += 1\n    return tn\ndef false_positive(y_true, y_pred):\n    \"\"\"\n    Function to calculate False Positives\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: number of false positives\n    \"\"\"\n    # initialize\n    fp = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 0 and yp == 1:\n            fp += 1\n    return fp\ndef false_negative(y_true, y_pred):\n    \"\"\"\n    Function to calculate False Negatives\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: number of false negatives\n    \"\"\"\n    # initialize\n    fn = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 1 and yp == 0:\n            fn += 1\n    return fn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l1 = [0,1,1,1,0,0,0,1]\nl2 = [0,1,0,1,0,1,0,0]\ntrue_positive(l1, l2), false_positive(l1, l2), false_negative(l1, l2), true_negative(l1, l2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy_v2(y_true, y_pred):\n    \"\"\"\n    Function to calculate accuracy using tp/tn/fp/fn\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: accuracy score\n    \"\"\"\n    tp = true_positive(y_true, y_pred)\n    fp = false_positive(y_true, y_pred)\n    fn = false_negative(y_true, y_pred)\n    tn = true_negative(y_true, y_pred)\n    accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n    return accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_v2(l1, l2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def precision(y_true, y_pred):\n    \"\"\"\n    Function to calculate precision\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: precision score\n    \"\"\"\n    tp = true_positive(y_true, y_pred)\n    fp = false_positive(y_true, y_pred)\n    precision = tp / (tp + fp)\n    return precision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision(l1, l2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recall(y_true, y_pred):\n    \"\"\"\n    Function to calculate recall\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: recall score\n    \"\"\"\n    tp = true_positive(y_true, y_pred)\n    fn = false_negative(y_true, y_pred)\n    recall = tp / (tp + fn)\n    return recall","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recall(l1, l2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n        1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\ny_pred = [0.02638412, 0.11114267, 0.31620708,\n            0.0490937, 0.0191491, 0.17554844,\n            0.15952202, 0.03819563, 0.11639273,\n            0.079377, 0.08584789, 0.39095342,\n            0.27259048, 0.03447096, 0.04644807,\n            0.03543574, 0.18521942, 0.05934905,\n            0.61977213, 0.33056815]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precisions = []\nrecalls = []\n# how we assumed these thresholds is a long story\nthresholds = [0.0490937 , 0.05934905, 0.079377,\n            0.08584789, 0.11114267, 0.11639273,\n            0.15952202, 0.17554844, 0.18521942,\n            0.27259048, 0.31620708, 0.33056815,\n            0.39095342, 0.61977213]\n# for every threshold, calculate predictions in binary\n# and append calculated precisions and recalls\n# to their respective lists\nfor i in thresholds:\n    temp_prediction = [1 if x >= i else 0 for x in y_pred]\n    p = precision(y_true, temp_prediction)\n    r = recall(y_true, temp_prediction)\n    precisions.append(p)\n    recalls.append(r)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nplt.plot(recalls, precisions)\nplt.xlabel('Recall', fontsize=15)\nplt.ylabel('Precision', fontsize=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1(y_true, y_pred):\n    \"\"\"\n    Function to calculate f1 score\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: f1 score\n    \"\"\"\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    score = 2 * p * r / (p + r)\n    return score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n            1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\ny_pred = [0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n            1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\nf1(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nmetrics.f1_score(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tpr(y_true, y_pred):\n    \"\"\"\n    Function to calculate tpr\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: tpr/recall\n    \"\"\"\n    return recall(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fpr(y_true, y_pred):\n    \"\"\"\n    Function to calculate fpr\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: fpr\n    \"\"\"\n    fp = false_positive(y_true, y_pred)\n    tn = true_negative(y_true, y_pred)\n    return fp / (tn + fp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# empty lists to store tpr\n# and fpr values\ntpr_list = []\nfpr_list = []\n# actual targets\ny_true = [0, 0, 0, 0, 1, 0, 1,\n0, 0, 1, 0, 1, 0, 0, 1]\n# predicted probabilities of a sample being 1\ny_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,\n0.9, 0.5, 0.3, 0.66, 0.3, 0.2,\n0.85, 0.15, 0.99]\n# handmade thresholds\nthresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5,\n0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0]\n# loop over all thresholds\nfor thresh in thresholds:\n    # calculate predictions for a given threshold\n    temp_pred = [1 if x >= thresh else 0 for x in y_pred]\n    # calculate tpr\n    temp_tpr = tpr(y_true, temp_pred)\n    # calculate fpr\n    temp_fpr = fpr(y_true, temp_pred)\n    # append tpr and fpr to lists\n    tpr_list.append(temp_tpr)\n    fpr_list.append(temp_fpr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nplt.fill_between(fpr_list, tpr_list, alpha=0.4)\nplt.plot(fpr_list, tpr_list, lw=3)\nplt.xlim(0, 1.0)\nplt.ylim(0, 1.0)\nplt.xlabel('FPR', fontsize=15)\nplt.ylabel('TPR', fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\ny_true = [0, 0, 0, 0, 1, 0, 1,\n        0, 0, 1, 0, 1, 0, 0, 1]\ny_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,\n        0.9, 0.5, 0.3, 0.66, 0.3, 0.2,\n        0.85, 0.15, 0.99]\nmetrics.roc_auc_score(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# empty lists to store true positive\n# and false positive values\ntp_list = []\nfp_list = []\n# actual targets\ny_true = [0, 0, 0, 0, 1, 0, 1,\n        0, 0, 1, 0, 1, 0, 0, 1]\n# predicted probabilities of a sample being 1\ny_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,\n        0.9, 0.5, 0.3, 0.66, 0.3, 0.2,\n        0.85, 0.15, 0.99]\n# some handmade thresholds\nthresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5,\n            0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0]\n# loop over all thresholds\nfor thresh in thresholds:\n    # calculate predictions for a given threshold\n    temp_pred = [1 if x >= thresh else 0 for x in y_pred]\n    # calculate tp\n    temp_tp = true_positive(y_true, temp_pred)\n    # calculate fp\n    temp_fp = false_positive(y_true, temp_pred)\n    # append tp and fp to lists\n    tp_list.append(temp_tp)\n    fp_list.append(temp_fp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndef log_loss(y_true, y_proba):\n    \"\"\"\n    Function to calculate log loss\n    :param y_true: list of true values\n    :param y_proba: list of probabilities for 1\n    :return: overall log loss\n    \"\"\"\n    # define an epsilon value\n    # this can also be an input\n    # this value is used to clip probabilities\n    epsilon = 1e-15\n    # initialize empty list to store\n    # individual losses\n    loss = []\n    # loop over all true and predicted probability values\n    for yt, yp in zip(y_true, y_proba):\n        # adjust probability\n        # 0 gets converted to 1e-15\n        # 1 gets converted to 1-1e-15\n        # Why? Think about it!\n        yp = np.clip(yp, epsilon, 1 - epsilon)\n        # calculate loss for one sample\n        temp_loss = - 1.0 * (\n        yt * np.log(yp)\n        + (1 - yt) * np.log(1 - yp)\n        )\n        # add to loss list\n        loss.append(temp_loss)\n    # return mean loss over all samples\n    return np.mean(loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = [0, 0, 0, 0, 1, 0, 1,\n            0, 0, 1, 0, 1, 0, 0, 1]\ny_proba = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,\n            0.9, 0.5, 0.3, 0.66, 0.3, 0.2,\n            0.85, 0.15, 0.99]\nlog_loss(y_true, y_proba)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nmetrics.log_loss(y_true, y_proba)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndef macro_precision(y_true, y_pred):\n    \"\"\"\n    Function to calculate macro averaged precision\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: macro precision score\n    \"\"\"\n    # find the number of classes by taking\n    # length of unique values in true list\n    num_classes = len(np.unique(y_true))\n    # initialize precision to 0\n    precision = 0\n    # loop over all classes\n    for class_ in range(num_classes):\n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        # calculate true positive for current class\n        tp = true_positive(temp_true, temp_pred)\n        # calculate false positive for current class\n        fp = false_positive(temp_true, temp_pred)\n        # calculate precision for current class\n        temp_precision = tp / (tp + fp)\n        # keep adding precision for all classes\n        precision += temp_precision\n    # calculate and return average precision over all classes\n    precision /= num_classes\n    return precision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndef micro_precision(y_true, y_pred):\n    \"\"\"\n    Function to calculate micro averaged precision\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: micro precision score\n    \"\"\"\n    # find the number of classes by taking\n    # length of unique values in true list\n    num_classes = len(np.unique(y_true))\n    # initialize tp and fp to 0\n    tp = 0\n    fp = 0\n    # loop over all classes\n    for class_ in range(num_classes):\n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        # calculate true positive for current class\n        # and update overall tp\n        tp += true_positive(temp_true, temp_pred)\n        # calculate false positive for current class\n        # and update overall tp\n        fp += false_positive(temp_true, temp_pred)\n    # calculate and return overall precision\n    precision = tp / (tp + fp)\n    return precision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nimport numpy as np\ndef weighted_precision(y_true, y_pred):\n    \"\"\"\n    Function to calculate weighted averaged precision\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: weighted precision score\n    \"\"\"\n    # find the number of classes by taking\n    # length of unique values in true list\n    num_classes = len(np.unique(y_true))\n    # create class:sample count dictionary\n    # it looks something like this:\n    # {0: 20, 1:15, 2:21}\n    class_counts = Counter(y_true)\n    # initialize precision to 0\n    precision = 0\n    # loop over all classes\n    for class_ in range(num_classes):\n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        # calculate tp and fp for class\n        tp = true_positive(temp_true, temp_pred)\n        fp = false_positive(temp_true, temp_pred)\n        # calculate precision of class\n        temp_precision = tp / (tp + fp)\n        # multiply precision with count of samples in class\n        weighted_precision = class_counts[class_] * temp_precision\n        # add to overall precision\n        precision += weighted_precision\n    # calculate overall precision by dividing by\n    # total number of samples\n    overall_precision = precision / len(y_true)\n    return overall_precision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\nprint(macro_precision(y_true, y_pred))\nprint(metrics.precision_score(y_true, y_pred, average=\"macro\"))\nprint(micro_precision(y_true, y_pred))\nprint(metrics.precision_score(y_true, y_pred, average=\"micro\"))\nprint(weighted_precision(y_true, y_pred))\nprint(metrics.precision_score(y_true, y_pred, average=\"weighted\"))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighted_f1(y_true, y_pred):\n    \"\"\"\n    Function to calculate weighted f1 score\n    :param y_true: list of true values\n    :param y_proba: list of predicted values\n    :return: weighted f1 score\n    \"\"\"\n    # find the number of classes by taking\n    # length of unique values in true list\n    num_classes = len(np.unique(y_true))\n    # create class:sample count dictionary\n    # it looks something like this:\n    # {0: 20, 1:15, 2:21}\n    class_counts = Counter(y_true)\n    # initialize f1 to 0\n    f1 = 0\n    # loop over all classes\n    for class_ in range(num_classes):\n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        # calculate precision and recall for class\n        p = precision(temp_true, temp_pred)\n        r = recall(temp_true, temp_pred)\n        # calculate f1 of class\n        if p + r != 0:\n            temp_f1 = 2 * p * r / (p + r)\n        else:\n            temp_f1 = 0\n        # multiply f1 with count of samples in class\n        weighted_f1 = class_counts[class_] * temp_f1\n        # add to f1 precision\n        f1 += weighted_f1\n    # calculate overall F1 by dividing by\n    # total number of samples\n    overall_f1 = f1 / len(y_true)\n    return overall_f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weighted_f1(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics.f1_score(y_true, y_pred, average=\"weighted\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\n# some targets\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n#some predictions\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n# get confusion matrix from sklearn\ncm = metrics.confusion_matrix(y_true, y_pred)\n# plot using matplotlib and seaborn\nplt.figure(figsize=(10, 10))\ncmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0,\nas_cmap=True)\nsns.set(font_scale=2.5)\nsns.heatmap(cm, annot=True, cmap=cmap, cbar=False)\nplt.ylabel('Actual Labels', fontsize=20)\nplt.xlabel('Predicted Labels', fontsize=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pk(y_true, y_pred, k):\n    \"\"\"\n    This function calculates precision at k\n    for a single sample\n    :param y_true: list of values, actual classes\n    :param y_pred: list of values, predicted classes\n    :param k: the value for k\n    :return: precision at a given value k\n    \"\"\"\n    # if k is 0, return 0. we should never have this\n    # as k is always >= 1\n    if k == 0:\n        return 0\n    # we are interested only in top-k predictions\n    y_pred = y_pred[:k]\n    # convert predictions to set\n    pred_set = set(y_pred)\n    # convert actual values to set\n    true_set = set(y_true)\n    # find common values\n    common_values = pred_set.intersection(true_set)\n    # return length of common values over k\n    return len(common_values) / len(y_pred[:k])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apk(y_true, y_pred, k):\n    \"\"\"\n    This function calculates average precision at k\n    for a single sample\n    :param y_true: list of values, actual classes\n    :param y_pred: list of values, predicted classes\n    :return: average precision at a given value k\n    \"\"\"\n    # initialize p@k list of values\n    pk_values = []\n    # loop over all k. from 1 to k + 1\n    for i in range(1, k + 1):\n        # calculate p@i and append to list\n        pk_values.append(pk(y_true, y_pred, i))\n    # if we have no values in the list, return 0\n    if len(pk_values) == 0:\n        return 0\n    # else, we return the sum of list over length of list\n    return sum(pk_values) / len(pk_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = [\n[1, 2, 3],\n[0, 2],\n[1],\n[2, 3],\n[1, 0],\n[]]\n\ny_pred = [\n[0, 1, 2],\n[1],\n[0, 2, 3],\n[2, 3, 4, 0],\n[0, 1, 2],\n[0]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(y_true)):\n    for j in range(1, 4):\n        print(\n\n        f\"\"\"\n        y_true={y_true[i]},\n        y_pred={y_pred[i]},\n        AP@{j}={apk(y_true[i], y_pred[i], k=j)}\n        \"\"\"\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mapk(y_true, y_pred, k):\n    \"\"\"\n    This function calculates mean avg precision at k\n    for a single sample\n    :param y_true: list of values, actual classes\n    :param y_pred: list of values, predicted classes\n    :return: mean avg precision at a given value k\n    \"\"\"\n    # initialize empty list for apk values\n    apk_values = []\n    # loop over all samples\n    for i in range(len(y_true)):\n    # store apk values for every sample\n        apk_values.append(\n        apk(y_true[i], y_pred[i], k=k)\n    )\n    # return mean of apk values list\n    return sum(apk_values) / len(apk_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mapk(y_true, y_pred, k=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# taken from:\n# https://github.com/benhamner/Metrics/blob/\n# master/Python/ml_metrics/average_precision.py\nimport numpy as np\ndef apk(actual, predicted, k=10):\n    \"\"\"\n    Computes the average precision at k.\n    This function computes the AP at k between two lists of\n    items.\n    Parameters\n    ----------\n    actual : list\n    A list of elements to be predicted (order doesn't matter)\n    predicted : list\n    A list of predicted elements (order does matter)\n    k : int, optional\n    The maximum number of predicted elements\n    Returns\n    -------\n    score : double\n    The average precision at k over the input lists\n    \"\"\"\n    if len(predicted)>k:\n        predicted = predicted[:k]\n    score = 0.0\n    num_hits = 0.0\n    for i,p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n    if not actual:\n        return 0.0\n    return score / min(len(actual), k)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndef r2(y_true, y_pred):\n    \"\"\"\n    This function calculates r-squared score\n    :param y_true: list of real numbers, true values\n    :param y_pred: list of real numbers, predicted values\n    :return: r2 score\n    \"\"\"\n    # calculate the mean value of true values\n    mean_true_value = np.mean(y_true)\n    # initialize numerator with 0\n    numerator = 0\n    # initialize denominator with 0\n    denominator = 0\n    # loop over all true and predicted values\n    for yt, yp in zip(y_true, y_pred):\n        # update numerator\n        numerator += (yt - yp) ** 2\n        # update denominator\n        denominator += (yt - mean_true_value) ** 2\n    # calculate the ratio\n    ratio = numerator / denominator\n    # return 1 - ratio\n    return (1 - ratio)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mapping = {\n    \"Freezing\": 0,\n    \"Warm\": 1,\n    \"Cold\": 2,\n    \"Boiling Hot\": 3,\n    \"Hot\": 4,\n    \"Lava Hot\": 5\n    }\n\nimport pandas as pd\ndf = pd.read_csv(\"/kaggle/input/aaamlp/cat_train.csv\")\ndf.loc[:, \"ord_2\"] = df.ord_2.map(mapping)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.ord_2.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn import preprocessing\n# read the data\ndf = pd.read_csv(\"/kaggle/input/aaamlp/cat_train.csv\")\n# fill NaN values in ord_2 column\ndf.loc[:, \"ord_2\"] = df.ord_2.fillna(\"NONE\")\n# initialize LabelEncoder\nlbl_enc = preprocessing.LabelEncoder()\n# fit label encoder and transform values on ord_2 column\n# P.S: do not use this directly. fit first, then transform\ndf.loc[:, \"ord_2\"] = lbl_enc.fit_transform(df.ord_2.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n# create our example feature matrix\nexample = np.array(\n[\n[0, 0, 1],\n[1, 0, 0],\n[1, 0, 1]\n]\n)\n# print size in bytes\nprint(example.nbytes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom scipy import sparse\n# create our example feature matrix\nexample = np.array(\n[\n[0, 0, 1],\n[1, 0, 0],\n[1, 0, 1]\n]\n)\n# convert numpy array to sparse CSR matrix\nsparse_example = sparse.csr_matrix(example)\n# print size of this sparse matrix\nprint(sparse_example.data.nbytes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\nsparse_example.data.nbytes +\nsparse_example.indptr.nbytes +\nsparse_example.indices.nbytes\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sparse_example.indptr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of rows\nn_rows = 10000\n# number of columns\nn_cols = 100000\n# create random binary matrix with only 5% values as 1s\nexample = np.random.binomial(1, p=0.05, size=(n_rows, n_cols))\n# print size in bytes\nprint(f\"Size of dense array: {example.nbytes}\")\n# convert numpy array to sparse CSR matrix\nsparse_example = sparse.csr_matrix(example)\n# print size of this sparse matrix\nprint(f\"Size of sparse array: {sparse_example.data.nbytes}\")\nfull_size = (\nsparse_example.data.nbytes +\nsparse_example.indptr.nbytes +\nsparse_example.indices.nbytes\n)\n# print full size of this sparse matrix\nprint(f\"Full size of sparse array: {full_size}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom scipy import sparse\n# create binary matrix\nexample = np.array(\n[\n[0, 0, 0, 0, 1, 0],\n[0, 1, 0, 0, 0, 0],\n[1, 0, 0, 0, 0, 0]\n]\n)\n# print size in bytes\nprint(f\"Size of dense array: {example.nbytes}\")\n# convert numpy array to sparse CSR matrix\nsparse_example = sparse.csr_matrix(example)\n# print size of this sparse matrix\nprint(f\"Size of sparse array: {sparse_example.data.nbytes}\")\nfull_size = (\nsparse_example.data.nbytes +\nsparse_example.indptr.nbytes +\nsparse_example.indices.nbytes\n)\n# print full size of this sparse matrix\nprint(f\"Full size of sparse array: {full_size}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn import preprocessing\n# create random 1-d array with 1001 different categories (int)\nexample = np.random.randint(1000, size=1000000)\n# initialize OneHotEncoder from scikit-learn\n# keep sparse = False to get dense array\nohe = preprocessing.OneHotEncoder(sparse=False)\n# fit and transform data with dense one hot encoder\nohe_example = ohe.fit_transform(example.reshape(-1, 1))\n# print size in bytes for dense array\nprint(f\"Size of dense array: {ohe_example.nbytes}\")\n# initialize OneHotEncoder from scikit-learn\n# keep sparse = True to get sparse array\nohe = preprocessing.OneHotEncoder(sparse=True)\n# fit and transform data with sparse one-hot encoder\nohe_example = ohe.fit_transform(example.reshape(-1, 1))\n# print size of this sparse matrix\nprint(f\"Size of sparse array: {ohe_example.data.nbytes}\")\nfull_size = (\nohe_example.data.nbytes +\nohe_example.indptr.nbytes + ohe_example.indices.nbytes\n)\n# print full size of this sparse matrix\nprint(f\"Full size of sparse array: {full_size}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/aaamlp/cat_train.csv\")\ndf[df.ord_2 == \"Boiling Hot\"].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby([\"ord_2\"])['id'].count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['ord_2'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby([\"ord_2\"])[\"id\"].transform(\"count\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(\n[\n\"ord_1\",\n\"ord_2\"\n])[\"id\"].count().reset_index(name=\"count\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"new_feature\"] = (\ndf.ord_1.astype(str)\n+ \"_\"\n+ df.ord_2.astype(str)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"new_feature\"].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.ord_2.fillna(\"NONE\").value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn import preprocessing\n# read training data\ntrain = pd.read_csv(\"/kaggle/input/aaamlp/cat_train.csv\")\n#read test data\ntest = pd.read_csv(\"/kaggle/input/aaamlp/cat_train.csv\")\n# create a fake target column for test data\n# since this column doesn't exist\ntest.loc[:, \"target\"] = -1\n# concatenate both training and test data\ndata = pd.concat([train, test]).reset_index(drop=True)\n# make a list of features we are interested in\n# id and target is something we should not encode\nfeatures = [x for x in train.columns if x not in [\"id\", \"target\"]]\n# loop over the features list\nfor feat in features:\n    # create a new instance of LabelEncoder for each feature\n    lbl_enc = preprocessing.LabelEncoder()\n    # note the trick here\n    # since its categorical data, we fillna with a string\n    # and we convert all the data to string type\n    # so, no matter its int or float, its converted to string\n    # int/float but categorical!!!\n    temp_col = data[feat].fillna(\"NONE\").astype(str).values\n    # we can use fit_transform here as we do not\n    # have any extra test data that we need to\n    # transform on separately\n    data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n# split the training and test data again\ntrain = data[data.target != -1].reset_index(drop=True)\ntest = data[data.target == -1].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.ord_4.fillna(\"NONE\").value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.ord_4 = df.ord_4.fillna(\"NONE\")\ndf.loc[df[\"ord_4\"].value_counts()[df[\"ord_4\"]].values < 2000,\"ord_4\"] = \"RARE\"\ndf.ord_4.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n# create a series of datetime with a frequency of 10 hours\ns = pd.date_range('2020-01-06', '2020-01-10', freq='10H').to_series()\n# create some features based on datetime\nfeatures = {\n    \"dayofweek\": s.dt.dayofweek.values,\n    \"dayofyear\": s.dt.dayofyear.values,\n    \"hour\": s.dt.hour.values,\n    \"is_leap_year\": s.dt.is_leap_year.values,\n    \"quarter\": s.dt.quarter.values,\n    \"weekofyear\": s.dt.weekofyear.values\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_features(df):\n    # create a bunch of features using the date column\n    df.loc[:, 'year'] = df['date'].dt.year\n    df.loc[:, 'weekofyear'] = df['date'].dt.weekofyear\n    df.loc[:, 'month'] = df['date'].dt.month\n    df.loc[:, 'dayofweek'] = df['date'].dt.dayofweek\n    df.loc[:, 'weekend'] = (df['date'].dt.weekday >=5).astype(int)\n    # create an aggregate dictionary\n    aggs = {}\n    # for aggregation by month, we calculate the\n    # number of unique month values and also the mean\n    aggs['month'] = ['nunique', 'mean']\n    aggs['weekofyear'] = ['nunique', 'mean']\n    # we aggregate by num1 and calculate sum, max, min\n    # and mean values of this column\n    aggs['num1'] = ['sum','max','min','mean']\n    # for customer_id, we calculate the total count\n    aggs['customer_id'] = ['size']\n    # again for customer_id, we calculate the total unique\n    aggs['customer_id'] = ['nunique']\n    # we group by customer_id and calculate the aggregates\n    agg_df = df.groupby('customer_id').agg(aggs)\n    agg_df = agg_df.reset_index()\n    return agg_df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfeature_dict = {}\n# calculate mean\nfeature_dict['mean'] = np.mean(x)\n# calculate max\nfeature_dict['max'] = np.max(x)\n# calculate min\nfeature_dict['min'] = np.min(x)\n# calculate standard deviation\nfeature_dict['std'] = np.std(x)\n# calculate variance\nfeature_dict['var'] = np.var(x)\n# peak-to-peak\nfeature_dict['ptp'] = np.ptp(x)\n# percentile features\nfeature_dict['percentile_10'] = np.percentile(x, 10)\nfeature_dict['percentile_60'] = np.percentile(x, 60)\nfeature_dict['percentile_90'] = np.percentile(x, 90)\n# quantile features\nfeature_dict['quantile_5'] = np.quantile(x, 0.05)\nfeature_dict['quantile_95'] = np.quantile(x, 0.95)\nfeature_dict['quantile_99'] = np.quantile(x, 0.99)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tsfresh.feature_extraction import feature_calculators as fc\n# tsfresh based features\nfeature_dict['abs_energy'] = fc.abs_energy(x)\nfeature_dict['count_above_mean'] = fc.count_above_mean(x)\nfeature_dict['count_below_mean'] = fc.count_below_mean(x)\nfeature_dict['mean_abs_change'] = fc.mean_abs_change(x)\nfeature_dict['mean_change'] = fc.mean_change(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n# generate a random dataframe with\n# 2 columns and 100 rows\ndf = pd.DataFrame(\nnp.random.rand(100, 2),\ncolumns=[f\"f_{i}\" for i in range(1, 3)]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\n# initialize polynomial features class object\n# for two-degree polynomial features\npf = preprocessing.PolynomialFeatures(\ndegree=2,\ninteraction_only=False,\ninclude_bias=False\n)\n# fit to the features\npf.fit(df)\n# create polynomial features\npoly_feats = pf.transform(df)\n# create a dataframe with all the features\nnum_feats = poly_feats.shape[1]\ndf_transformed = pd.DataFrame(\npoly_feats,\ncolumns=[f\"f_{i}\" for i in range(1, num_feats + 1)]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create bins of the numerical columns\n# 10 bins\ndf[\"f_bin_10\"] = pd.cut(df[\"f_1\"], bins=10, labels=False)\n# 100 bins\ndf[\"f_bin_100\"] = pd.cut(df[\"f_1\"], bins=100, labels=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_transformed.f_14.var()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_transformed.f_14.apply(lambda x: np.log(1 + x)).var()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn import impute\n# create a random numpy array with 10 samples\n# and 6 features and values ranging from 1 to 15\nX = np.random.randint(1, 15, (10, 6))\n# convert the array to float\nX = X.astype(float)\n# randomly assign 10 elements to NaN (missing)\nX.ravel()[np.random.choice(X.size, 10, replace=False)] = np.nan\n# use 2 nearest neighbours to fill na values\nknn_imputer = impute.KNNImputer(n_neighbors=2)\nknn_imputer.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\ndata = df\nvar_thresh = VarianceThreshold(threshold=0.1)\ntransformed_data = var_thresh.fit_transform(data)\n# transformed data will have all columns with variance less\n# than 0.1 removed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.datasets import fetch_california_housing\n# fetch a regression dataset\ndata = fetch_california_housing()\nX = data[\"data\"]\ncol_names = data[\"feature_names\"]\ny = data[\"target\"]\n# convert to pandas dataframe\ndf = pd.DataFrame(X, columns=col_names)\n# introduce a highly correlated column\ndf.loc[:, \"MedInc_Sqrt\"] = df.MedInc.apply(np.sqrt)\n# get correlation matrix (pearson)\ndf.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import SelectPercentile\n\n\nclass UnivariateFeatureSelction:\n    def __init__(self, n_features, problem_type, scoring):\n        \"\"\"\n        Custom univariate feature selection wrapper on\n        different univariate feature selection models from\n        scikit-learn.\n        :param n_features: SelectPercentile if float else SelectKBest\n        :param problem_type: classification or regression\n        :param scoring: scoring function, string\n        \"\"\"\n        # for a given problem type, there are only\n        # a few valid scoring methods\n        # you can extend this with your own custom\n        # methods if you wish\n        if problem_type == \"classification\":\n            valid_scoring = {\n            \"f_classif\": f_classif,\n            \"chi2\": chi2,\n            \"mutual_info_classif\": mutual_info_classif\n            }\n        else:\n            valid_scoring = {\n            \"f_regression\": f_regression,\n            \"mutual_info_regression\": mutual_info_regression\n            }\n        # raise exception if we do not have a valid scoring method\n        if scoring not in valid_scoring:\n            raise Exception(\"Invalid scoring function\")\n        # if n_features is int, we use selectkbest\n        # if n_features is float, we use selectpercentile\n        # please note that it is int in both cases in sklearn\n        if isinstance(n_features, int):\n            self.selection = SelectKBest(\n            valid_scoring[scoring],\n            k=n_features\n            )\n        elif isinstance(n_features, float):\n            self.selection = SelectPercentile(\n            valid_scoring[scoring],\n            percentile=int(n_features * 100)\n            )\n        else:\n            raise Exception(\"Invalid type of feature\")\n    \n    # same fit function\n    def fit(self, X, y):\n        return self.selection.fit(X, y)\n    \n    # same transform function\n    def transform(self, X):\n        return self.selection.transform(X)\n    \n    # same fit_transform function\n    def fit_transform(self, X, y):\n        return self.selection.fit_transform(X, y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ufs = UnivariateFeatureSelction(\n    n_features=5,\n    problem_type=\"regression\",\n    scoring=\"f_regression\"\n)\nufs.fit(X, y)\nX_transformed = ufs.transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_transformed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# greedy.py\nimport pandas as pd\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn.datasets import make_classification\nclass GreedyFeatureSelection:\n    \"\"\"\n    A simple and custom class for greedy feature selection.\n    You will need to modify it quite a bit to make it suitable\n    for your dataset.\n    \"\"\"\n    def evaluate_score(self, X, y):\n        \"\"\"\n        This function evaluates model on data and returns\n        Area Under ROC Curve (AUC)\n        NOTE: We fit the data and calculate AUC on same data.\n        WE ARE OVERFITTING HERE.\n        But this is also a way to achieve greedy selection.\n        k-fold will take k times longer.\n        If you want to implement it in really correct way,\n        calculate OOF AUC and return mean AUC over k folds.\n        This requires only a few lines of change and has been\n        shown a few times in this book.\n        :param X: training data\n        :param y: targets\n        :return: overfitted area under the roc curve\n        \"\"\"\n        # fit the logistic regression model,\n        # and calculate AUC on same data\n        # again: BEWARE\n        # you can choose any model that suits your data\n        model = linear_model.LogisticRegression()\n        model.fit(X, y)\n        predictions = model.predict_proba(X)[:, 1]\n        auc = metrics.roc_auc_score(y, predictions)\n        return auc\n    \n    def _feature_selection(self, X, y):\n        \"\"\"\n        This function does the actual greedy selection\n        :param X: data, numpy array\n        :param y: targets, numpy array\n        :return: (best scores, best features)\n        \"\"\"\n        # initialize good features list\n        # and best scores to keep track of both\n        good_features = []\n        best_scores = []\n        # calculate the number of features\n        num_features = X.shape[1]\n        # infinite loop\n        while True:\n            # initialize best feature and score of this loop\n            this_feature = None\n            best_score = 0\n            # loop over all features\n            for feature in range(num_features):\n                # if feature is already in good features,\n                # skip this for loop\n                if feature in good_features:\n                    continue\n                # selected features are all good features till now\n                # and current feature\n                selected_features = good_features + [feature]\n                # remove all other features from data\n                xtrain = X[:, selected_features]\n                # calculate the score, in our case, AUC\n                score = self.evaluate_score(xtrain, y)\n                # if score is greater than the best score\n                # of this loop, change best score and best feature\n                if score > best_score:\n                    this_feature = feature\n                    best_score = score\n            # if we have selected a feature, add it\n            # to the good feature list and update best scores list\n            if this_feature != None:\n                good_features.append(this_feature)\n                best_scores.append(best_score)\n            # if we didnt improve during the previous round,\n            # exit the while loop\n            if len(best_scores) > 2:\n                if best_scores[-1] < best_scores[-2]:\n                    break\n        # return best scores and good features\n        # why do we remove the last data point?\n        return best_scores[:-1], good_features[:-1]\n    \n    def __call__(self, X, y):\n        \"\"\"\n        Call function will call the class on a set of arguments\n        \"\"\"\n        # select features, return scores and selected indices\n        scores, features = self._feature_selection(X, y)\n        # transform data with selected features\n        return X[:, features], scores\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # generate binary classification data\n    X, y = make_classification(n_samples=1000, n_features=100)\n    # transform data by greedy feature selection\n    X_transformed, scores = GreedyFeatureSelection()(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_transformed.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import fetch_california_housing\n# fetch a regression dataset\ndata = fetch_california_housing()\nX = data[\"data\"]\ncol_names = data[\"feature_names\"]\ny = data[\"target\"]\n# initialize the model\nmodel = LinearRegression()\n# initialize RFE\nrfe = RFE(\nestimator=model,\nn_features_to_select=3\n)\n# fit RFE\nrfe.fit(X, y)\n# get the transformed data with\n# selected columns\nX_transformed = rfe.transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_transformed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n# fetch a regression dataset\n# in diabetes data we predict diabetes progression\n# after one year based on some features\ndata = load_diabetes()\nX = data[\"data\"]\ncol_names = data[\"feature_names\"]\ny = data[\"target\"]\n# initialize the model\nmodel = RandomForestRegressor()\n# fit the model\nmodel.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = model.feature_importances_\nidxs = np.argsort(importances)\nplt.title('Feature Importances')\nplt.barh(range(len(idxs)), importances[idxs], align='center')\nplt.yticks(range(len(idxs)), [col_names[i] for i in idxs])\nplt.xlabel('Random Forest Feature Importance')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import SelectFromModel\n# fetch a regression dataset\n# in diabetes data we predict diabetes progression\n# after one year based on some features\ndata = load_diabetes()\nX = data[\"data\"]\ncol_names = data[\"feature_names\"]\ny = data[\"target\"]\n# initialize the model\nmodel = RandomForestRegressor()\n# select from the model\nsfm = SelectFromModel(estimator=model)\nX_transformed = sfm.fit_transform(X, y)\n# see which features were selected\nsupport = sfm.get_support()\n# get feature names\nprint([\n    x for x, y in zip(col_names, support) if y == True\n])","metadata":{"execution":{"iopub.status.busy":"2021-05-21T06:17:17.660868Z","iopub.execute_input":"2021-05-21T06:17:17.661331Z","iopub.status.idle":"2021-05-21T06:17:19.653503Z","shell.execute_reply.started":"2021-05-21T06:17:17.661236Z","shell.execute_reply":"2021-05-21T06:17:19.652118Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"['bmi', 's5']\n","output_type":"stream"}]},{"cell_type":"code","source":"# rf_grid_search.py\nimport numpy as np\nimport pandas as pd\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn import model_selection\n\nif __name__ == \"__main__\":\n    # read the training data\n    df = pd.read_csv(\"/kaggle/input/aaamlp/mobile_train.csv\")\n    # features are all columns without price_range\n    # note that there is no id column in this dataset\n    # here we have training features\n    X = df.drop(\"price_range\", axis=1).values\n    # and the targets\n    y = df.price_range.values\n    # define the model here\n    # i am using random forest with n_jobs=-1\n    # n_jobs=-1 => use all cores\n    classifier = ensemble.RandomForestClassifier(n_jobs=-1)\n    # define a grid of parameters\n    # this can be a dictionary or a list of\n    # dictionaries\n    param_grid = {\n        \"n_estimators\": [100, 200, 250, 300, 400, 500],\n        \"max_depth\": [1, 2, 5, 7, 11, 15],\n        \"criterion\": [\"gini\", \"entropy\"]\n    }\n    # initialize grid search\n    # estimator is the model that we have defined\n    # param_grid is the grid of parameters\n    # we use accuracy as our metric. you can define your own\n    # higher value of verbose implies a lot of details are printed\n    # cv=5 means that we are using 5 fold cv (not stratified)\n    model = model_selection.GridSearchCV(\n    estimator=classifier,\n    param_grid=param_grid,\n    scoring=\"accuracy\",\n    verbose=10,\n    n_jobs=1,\n    cv=5\n    )\n    # fit the model and extract best score\n    model.fit(X, y)\n    print(f\"Best score: {model.best_score_}\")\n    print(\"Best parameters set:\")\n    best_parameters = model.best_estimator_.get_params()\n    for param_name in sorted(param_grid.keys()):\n        print(f\"\\t{param_name}: {best_parameters[param_name]}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-21T06:31:56.463386Z","iopub.execute_input":"2021-05-21T06:31:56.463773Z","iopub.status.idle":"2021-05-21T06:37:01.859966Z","shell.execute_reply.started":"2021-05-21T06:31:56.463733Z","shell.execute_reply":"2021-05-21T06:37:01.859052Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 72 candidates, totalling 360 fits\n[CV 1/5; 1/72] START criterion=gini, max_depth=1, n_estimators=100..............\n[CV 1/5; 1/72] END criterion=gini, max_depth=1, n_estimators=100; total time=   2.2s\n[CV 2/5; 1/72] START criterion=gini, max_depth=1, n_estimators=100..............\n[CV 2/5; 1/72] END criterion=gini, max_depth=1, n_estimators=100; total time=   0.3s\n[CV 3/5; 1/72] START criterion=gini, max_depth=1, n_estimators=100..............\n[CV 3/5; 1/72] END criterion=gini, max_depth=1, n_estimators=100; total time=   0.4s\n[CV 4/5; 1/72] START criterion=gini, max_depth=1, n_estimators=100..............\n[CV 4/5; 1/72] END criterion=gini, max_depth=1, n_estimators=100; total time=   0.4s\n[CV 5/5; 1/72] START criterion=gini, max_depth=1, n_estimators=100..............\n[CV 5/5; 1/72] END criterion=gini, max_depth=1, n_estimators=100; total time=   0.4s\n[CV 1/5; 2/72] START criterion=gini, max_depth=1, n_estimators=200..............\n[CV 1/5; 2/72] END criterion=gini, max_depth=1, n_estimators=200; total time=   0.5s\n[CV 2/5; 2/72] START criterion=gini, max_depth=1, n_estimators=200..............\n[CV 2/5; 2/72] END criterion=gini, max_depth=1, n_estimators=200; total time=   0.5s\n[CV 3/5; 2/72] START criterion=gini, max_depth=1, n_estimators=200..............\n[CV 3/5; 2/72] END criterion=gini, max_depth=1, n_estimators=200; total time=   0.5s\n[CV 4/5; 2/72] START criterion=gini, max_depth=1, n_estimators=200..............\n[CV 4/5; 2/72] END criterion=gini, max_depth=1, n_estimators=200; total time=   0.5s\n[CV 5/5; 2/72] START criterion=gini, max_depth=1, n_estimators=200..............\n[CV 5/5; 2/72] END criterion=gini, max_depth=1, n_estimators=200; total time=   0.5s\n[CV 1/5; 3/72] START criterion=gini, max_depth=1, n_estimators=250..............\n[CV 1/5; 3/72] END criterion=gini, max_depth=1, n_estimators=250; total time=   0.5s\n[CV 2/5; 3/72] START criterion=gini, max_depth=1, n_estimators=250..............\n[CV 2/5; 3/72] END criterion=gini, max_depth=1, n_estimators=250; total time=   0.5s\n[CV 3/5; 3/72] START criterion=gini, max_depth=1, n_estimators=250..............\n[CV 3/5; 3/72] END criterion=gini, max_depth=1, n_estimators=250; total time=   0.6s\n[CV 4/5; 3/72] START criterion=gini, max_depth=1, n_estimators=250..............\n[CV 4/5; 3/72] END criterion=gini, max_depth=1, n_estimators=250; total time=   0.5s\n[CV 5/5; 3/72] START criterion=gini, max_depth=1, n_estimators=250..............\n[CV 5/5; 3/72] END criterion=gini, max_depth=1, n_estimators=250; total time=   0.5s\n[CV 1/5; 4/72] START criterion=gini, max_depth=1, n_estimators=300..............\n[CV 1/5; 4/72] END criterion=gini, max_depth=1, n_estimators=300; total time=   0.8s\n[CV 2/5; 4/72] START criterion=gini, max_depth=1, n_estimators=300..............\n[CV 2/5; 4/72] END criterion=gini, max_depth=1, n_estimators=300; total time=   0.7s\n[CV 3/5; 4/72] START criterion=gini, max_depth=1, n_estimators=300..............\n[CV 3/5; 4/72] END criterion=gini, max_depth=1, n_estimators=300; total time=   0.8s\n[CV 4/5; 4/72] START criterion=gini, max_depth=1, n_estimators=300..............\n[CV 4/5; 4/72] END criterion=gini, max_depth=1, n_estimators=300; total time=   0.8s\n[CV 5/5; 4/72] START criterion=gini, max_depth=1, n_estimators=300..............\n[CV 5/5; 4/72] END criterion=gini, max_depth=1, n_estimators=300; total time=   0.7s\n[CV 1/5; 5/72] START criterion=gini, max_depth=1, n_estimators=400..............\n[CV 1/5; 5/72] END criterion=gini, max_depth=1, n_estimators=400; total time=   0.8s\n[CV 2/5; 5/72] START criterion=gini, max_depth=1, n_estimators=400..............\n[CV 2/5; 5/72] END criterion=gini, max_depth=1, n_estimators=400; total time=   0.9s\n[CV 3/5; 5/72] START criterion=gini, max_depth=1, n_estimators=400..............\n[CV 3/5; 5/72] END criterion=gini, max_depth=1, n_estimators=400; total time=   1.0s\n[CV 4/5; 5/72] START criterion=gini, max_depth=1, n_estimators=400..............\n[CV 4/5; 5/72] END criterion=gini, max_depth=1, n_estimators=400; total time=   0.9s\n[CV 5/5; 5/72] START criterion=gini, max_depth=1, n_estimators=400..............\n[CV 5/5; 5/72] END criterion=gini, max_depth=1, n_estimators=400; total time=   1.0s\n[CV 1/5; 6/72] START criterion=gini, max_depth=1, n_estimators=500..............\n[CV 1/5; 6/72] END criterion=gini, max_depth=1, n_estimators=500; total time=   1.1s\n[CV 2/5; 6/72] START criterion=gini, max_depth=1, n_estimators=500..............\n[CV 2/5; 6/72] END criterion=gini, max_depth=1, n_estimators=500; total time=   1.0s\n[CV 3/5; 6/72] START criterion=gini, max_depth=1, n_estimators=500..............\n[CV 3/5; 6/72] END criterion=gini, max_depth=1, n_estimators=500; total time=   1.1s\n[CV 4/5; 6/72] START criterion=gini, max_depth=1, n_estimators=500..............\n[CV 4/5; 6/72] END criterion=gini, max_depth=1, n_estimators=500; total time=   1.0s\n[CV 5/5; 6/72] START criterion=gini, max_depth=1, n_estimators=500..............\n[CV 5/5; 6/72] END criterion=gini, max_depth=1, n_estimators=500; total time=   1.1s\n[CV 1/5; 7/72] START criterion=gini, max_depth=2, n_estimators=100..............\n[CV 1/5; 7/72] END criterion=gini, max_depth=2, n_estimators=100; total time=   0.4s\n[CV 2/5; 7/72] START criterion=gini, max_depth=2, n_estimators=100..............\n[CV 2/5; 7/72] END criterion=gini, max_depth=2, n_estimators=100; total time=   0.3s\n[CV 3/5; 7/72] START criterion=gini, max_depth=2, n_estimators=100..............\n[CV 3/5; 7/72] END criterion=gini, max_depth=2, n_estimators=100; total time=   0.3s\n[CV 4/5; 7/72] START criterion=gini, max_depth=2, n_estimators=100..............\n[CV 4/5; 7/72] END criterion=gini, max_depth=2, n_estimators=100; total time=   0.4s\n[CV 5/5; 7/72] START criterion=gini, max_depth=2, n_estimators=100..............\n[CV 5/5; 7/72] END criterion=gini, max_depth=2, n_estimators=100; total time=   0.3s\n[CV 1/5; 8/72] START criterion=gini, max_depth=2, n_estimators=200..............\n[CV 1/5; 8/72] END criterion=gini, max_depth=2, n_estimators=200; total time=   0.5s\n[CV 2/5; 8/72] START criterion=gini, max_depth=2, n_estimators=200..............\n[CV 2/5; 8/72] END criterion=gini, max_depth=2, n_estimators=200; total time=   0.5s\n[CV 3/5; 8/72] START criterion=gini, max_depth=2, n_estimators=200..............\n[CV 3/5; 8/72] END criterion=gini, max_depth=2, n_estimators=200; total time=   0.5s\n[CV 4/5; 8/72] START criterion=gini, max_depth=2, n_estimators=200..............\n[CV 4/5; 8/72] END criterion=gini, max_depth=2, n_estimators=200; total time=   0.5s\n[CV 5/5; 8/72] START criterion=gini, max_depth=2, n_estimators=200..............\n[CV 5/5; 8/72] END criterion=gini, max_depth=2, n_estimators=200; total time=   0.5s\n[CV 1/5; 9/72] START criterion=gini, max_depth=2, n_estimators=250..............\n[CV 1/5; 9/72] END criterion=gini, max_depth=2, n_estimators=250; total time=   0.6s\n[CV 2/5; 9/72] START criterion=gini, max_depth=2, n_estimators=250..............\n[CV 2/5; 9/72] END criterion=gini, max_depth=2, n_estimators=250; total time=   0.6s\n[CV 3/5; 9/72] START criterion=gini, max_depth=2, n_estimators=250..............\n[CV 3/5; 9/72] END criterion=gini, max_depth=2, n_estimators=250; total time=   0.5s\n[CV 4/5; 9/72] START criterion=gini, max_depth=2, n_estimators=250..............\n[CV 4/5; 9/72] END criterion=gini, max_depth=2, n_estimators=250; total time=   0.6s\n[CV 5/5; 9/72] START criterion=gini, max_depth=2, n_estimators=250..............\n[CV 5/5; 9/72] END criterion=gini, max_depth=2, n_estimators=250; total time=   0.6s\n[CV 1/5; 10/72] START criterion=gini, max_depth=2, n_estimators=300.............\n[CV 1/5; 10/72] END criterion=gini, max_depth=2, n_estimators=300; total time=   0.8s\n[CV 2/5; 10/72] START criterion=gini, max_depth=2, n_estimators=300.............\n[CV 2/5; 10/72] END criterion=gini, max_depth=2, n_estimators=300; total time=   0.7s\n[CV 3/5; 10/72] START criterion=gini, max_depth=2, n_estimators=300.............\n[CV 3/5; 10/72] END criterion=gini, max_depth=2, n_estimators=300; total time=   0.8s\n[CV 4/5; 10/72] START criterion=gini, max_depth=2, n_estimators=300.............\n[CV 4/5; 10/72] END criterion=gini, max_depth=2, n_estimators=300; total time=   0.8s\n[CV 5/5; 10/72] START criterion=gini, max_depth=2, n_estimators=300.............\n[CV 5/5; 10/72] END criterion=gini, max_depth=2, n_estimators=300; total time=   0.8s\n[CV 1/5; 11/72] START criterion=gini, max_depth=2, n_estimators=400.............\n[CV 1/5; 11/72] END criterion=gini, max_depth=2, n_estimators=400; total time=   1.0s\n[CV 2/5; 11/72] START criterion=gini, max_depth=2, n_estimators=400.............\n[CV 2/5; 11/72] END criterion=gini, max_depth=2, n_estimators=400; total time=   0.9s\n[CV 3/5; 11/72] START criterion=gini, max_depth=2, n_estimators=400.............\n[CV 3/5; 11/72] END criterion=gini, max_depth=2, n_estimators=400; total time=   0.9s\n[CV 4/5; 11/72] START criterion=gini, max_depth=2, n_estimators=400.............\n[CV 4/5; 11/72] END criterion=gini, max_depth=2, n_estimators=400; total time=   0.9s\n[CV 5/5; 11/72] START criterion=gini, max_depth=2, n_estimators=400.............\n[CV 5/5; 11/72] END criterion=gini, max_depth=2, n_estimators=400; total time=   0.9s\n[CV 1/5; 12/72] START criterion=gini, max_depth=2, n_estimators=500.............\n[CV 1/5; 12/72] END criterion=gini, max_depth=2, n_estimators=500; total time=   1.1s\n[CV 2/5; 12/72] START criterion=gini, max_depth=2, n_estimators=500.............\n[CV 2/5; 12/72] END criterion=gini, max_depth=2, n_estimators=500; total time=   1.1s\n[CV 3/5; 12/72] START criterion=gini, max_depth=2, n_estimators=500.............\n[CV 3/5; 12/72] END criterion=gini, max_depth=2, n_estimators=500; total time=   1.0s\n[CV 4/5; 12/72] START criterion=gini, max_depth=2, n_estimators=500.............\n[CV 4/5; 12/72] END criterion=gini, max_depth=2, n_estimators=500; total time=   1.1s\n[CV 5/5; 12/72] START criterion=gini, max_depth=2, n_estimators=500.............\n[CV 5/5; 12/72] END criterion=gini, max_depth=2, n_estimators=500; total time=   1.1s\n[CV 1/5; 13/72] START criterion=gini, max_depth=5, n_estimators=100.............\n[CV 1/5; 13/72] END criterion=gini, max_depth=5, n_estimators=100; total time=   0.4s\n[CV 2/5; 13/72] START criterion=gini, max_depth=5, n_estimators=100.............\n[CV 2/5; 13/72] END criterion=gini, max_depth=5, n_estimators=100; total time=   0.4s\n[CV 3/5; 13/72] START criterion=gini, max_depth=5, n_estimators=100.............\n[CV 3/5; 13/72] END criterion=gini, max_depth=5, n_estimators=100; total time=   0.3s\n[CV 4/5; 13/72] START criterion=gini, max_depth=5, n_estimators=100.............\n[CV 4/5; 13/72] END criterion=gini, max_depth=5, n_estimators=100; total time=   0.4s\n[CV 5/5; 13/72] START criterion=gini, max_depth=5, n_estimators=100.............\n[CV 5/5; 13/72] END criterion=gini, max_depth=5, n_estimators=100; total time=   0.4s\n[CV 1/5; 14/72] START criterion=gini, max_depth=5, n_estimators=200.............\n[CV 1/5; 14/72] END criterion=gini, max_depth=5, n_estimators=200; total time=   0.5s\n[CV 2/5; 14/72] START criterion=gini, max_depth=5, n_estimators=200.............\n[CV 2/5; 14/72] END criterion=gini, max_depth=5, n_estimators=200; total time=   0.5s\n[CV 3/5; 14/72] START criterion=gini, max_depth=5, n_estimators=200.............\n[CV 3/5; 14/72] END criterion=gini, max_depth=5, n_estimators=200; total time=   0.5s\n[CV 4/5; 14/72] START criterion=gini, max_depth=5, n_estimators=200.............\n[CV 4/5; 14/72] END criterion=gini, max_depth=5, n_estimators=200; total time=   0.6s\n[CV 5/5; 14/72] START criterion=gini, max_depth=5, n_estimators=200.............\n[CV 5/5; 14/72] END criterion=gini, max_depth=5, n_estimators=200; total time=   0.5s\n[CV 1/5; 15/72] START criterion=gini, max_depth=5, n_estimators=250.............\n[CV 1/5; 15/72] END criterion=gini, max_depth=5, n_estimators=250; total time=   0.6s\n[CV 2/5; 15/72] START criterion=gini, max_depth=5, n_estimators=250.............\n[CV 2/5; 15/72] END criterion=gini, max_depth=5, n_estimators=250; total time=   0.6s\n[CV 3/5; 15/72] START criterion=gini, max_depth=5, n_estimators=250.............\n[CV 3/5; 15/72] END criterion=gini, max_depth=5, n_estimators=250; total time=   0.6s\n[CV 4/5; 15/72] START criterion=gini, max_depth=5, n_estimators=250.............\n[CV 4/5; 15/72] END criterion=gini, max_depth=5, n_estimators=250; total time=   0.6s\n[CV 5/5; 15/72] START criterion=gini, max_depth=5, n_estimators=250.............\n[CV 5/5; 15/72] END criterion=gini, max_depth=5, n_estimators=250; total time=   0.6s\n[CV 1/5; 16/72] START criterion=gini, max_depth=5, n_estimators=300.............\n[CV 1/5; 16/72] END criterion=gini, max_depth=5, n_estimators=300; total time=   0.8s\n[CV 2/5; 16/72] START criterion=gini, max_depth=5, n_estimators=300.............\n[CV 2/5; 16/72] END criterion=gini, max_depth=5, n_estimators=300; total time=   0.8s\n[CV 3/5; 16/72] START criterion=gini, max_depth=5, n_estimators=300.............\n[CV 3/5; 16/72] END criterion=gini, max_depth=5, n_estimators=300; total time=   0.8s\n[CV 4/5; 16/72] START criterion=gini, max_depth=5, n_estimators=300.............\n[CV 4/5; 16/72] END criterion=gini, max_depth=5, n_estimators=300; total time=   0.8s\n[CV 5/5; 16/72] START criterion=gini, max_depth=5, n_estimators=300.............\n[CV 5/5; 16/72] END criterion=gini, max_depth=5, n_estimators=300; total time=   0.8s\n[CV 1/5; 17/72] START criterion=gini, max_depth=5, n_estimators=400.............\n[CV 1/5; 17/72] END criterion=gini, max_depth=5, n_estimators=400; total time=   1.2s\n[CV 2/5; 17/72] START criterion=gini, max_depth=5, n_estimators=400.............\n[CV 2/5; 17/72] END criterion=gini, max_depth=5, n_estimators=400; total time=   1.0s\n[CV 3/5; 17/72] START criterion=gini, max_depth=5, n_estimators=400.............\n[CV 3/5; 17/72] END criterion=gini, max_depth=5, n_estimators=400; total time=   1.0s\n[CV 4/5; 17/72] START criterion=gini, max_depth=5, n_estimators=400.............\n[CV 4/5; 17/72] END criterion=gini, max_depth=5, n_estimators=400; total time=   1.0s\n[CV 5/5; 17/72] START criterion=gini, max_depth=5, n_estimators=400.............\n[CV 5/5; 17/72] END criterion=gini, max_depth=5, n_estimators=400; total time=   1.0s\n[CV 1/5; 18/72] START criterion=gini, max_depth=5, n_estimators=500.............\n[CV 1/5; 18/72] END criterion=gini, max_depth=5, n_estimators=500; total time=   1.2s\n[CV 2/5; 18/72] START criterion=gini, max_depth=5, n_estimators=500.............\n[CV 2/5; 18/72] END criterion=gini, max_depth=5, n_estimators=500; total time=   1.2s\n[CV 3/5; 18/72] START criterion=gini, max_depth=5, n_estimators=500.............\n[CV 3/5; 18/72] END criterion=gini, max_depth=5, n_estimators=500; total time=   1.2s\n[CV 4/5; 18/72] START criterion=gini, max_depth=5, n_estimators=500.............\n[CV 4/5; 18/72] END criterion=gini, max_depth=5, n_estimators=500; total time=   1.2s\n[CV 5/5; 18/72] START criterion=gini, max_depth=5, n_estimators=500.............\n[CV 5/5; 18/72] END criterion=gini, max_depth=5, n_estimators=500; total time=   1.2s\n[CV 1/5; 19/72] START criterion=gini, max_depth=7, n_estimators=100.............\n[CV 1/5; 19/72] END criterion=gini, max_depth=7, n_estimators=100; total time=   0.4s\n[CV 2/5; 19/72] START criterion=gini, max_depth=7, n_estimators=100.............\n[CV 2/5; 19/72] END criterion=gini, max_depth=7, n_estimators=100; total time=   0.4s\n[CV 3/5; 19/72] START criterion=gini, max_depth=7, n_estimators=100.............\n[CV 3/5; 19/72] END criterion=gini, max_depth=7, n_estimators=100; total time=   0.4s\n[CV 4/5; 19/72] START criterion=gini, max_depth=7, n_estimators=100.............\n[CV 4/5; 19/72] END criterion=gini, max_depth=7, n_estimators=100; total time=   0.4s\n[CV 5/5; 19/72] START criterion=gini, max_depth=7, n_estimators=100.............\n[CV 5/5; 19/72] END criterion=gini, max_depth=7, n_estimators=100; total time=   0.4s\n[CV 1/5; 20/72] START criterion=gini, max_depth=7, n_estimators=200.............\n[CV 1/5; 20/72] END criterion=gini, max_depth=7, n_estimators=200; total time=   0.6s\n[CV 2/5; 20/72] START criterion=gini, max_depth=7, n_estimators=200.............\n[CV 2/5; 20/72] END criterion=gini, max_depth=7, n_estimators=200; total time=   0.6s\n[CV 3/5; 20/72] START criterion=gini, max_depth=7, n_estimators=200.............\n[CV 3/5; 20/72] END criterion=gini, max_depth=7, n_estimators=200; total time=   0.5s\n[CV 4/5; 20/72] START criterion=gini, max_depth=7, n_estimators=200.............\n[CV 4/5; 20/72] END criterion=gini, max_depth=7, n_estimators=200; total time=   0.6s\n[CV 5/5; 20/72] START criterion=gini, max_depth=7, n_estimators=200.............\n[CV 5/5; 20/72] END criterion=gini, max_depth=7, n_estimators=200; total time=   0.6s\n[CV 1/5; 21/72] START criterion=gini, max_depth=7, n_estimators=250.............\n[CV 1/5; 21/72] END criterion=gini, max_depth=7, n_estimators=250; total time=   0.7s\n[CV 2/5; 21/72] START criterion=gini, max_depth=7, n_estimators=250.............\n[CV 2/5; 21/72] END criterion=gini, max_depth=7, n_estimators=250; total time=   0.7s\n[CV 3/5; 21/72] START criterion=gini, max_depth=7, n_estimators=250.............\n[CV 3/5; 21/72] END criterion=gini, max_depth=7, n_estimators=250; total time=   0.6s\n[CV 4/5; 21/72] START criterion=gini, max_depth=7, n_estimators=250.............\n[CV 4/5; 21/72] END criterion=gini, max_depth=7, n_estimators=250; total time=   0.7s\n[CV 5/5; 21/72] START criterion=gini, max_depth=7, n_estimators=250.............\n[CV 5/5; 21/72] END criterion=gini, max_depth=7, n_estimators=250; total time=   0.7s\n[CV 1/5; 22/72] START criterion=gini, max_depth=7, n_estimators=300.............\n[CV 1/5; 22/72] END criterion=gini, max_depth=7, n_estimators=300; total time=   0.8s\n[CV 2/5; 22/72] START criterion=gini, max_depth=7, n_estimators=300.............\n[CV 2/5; 22/72] END criterion=gini, max_depth=7, n_estimators=300; total time=   0.9s\n[CV 3/5; 22/72] START criterion=gini, max_depth=7, n_estimators=300.............\n[CV 3/5; 22/72] END criterion=gini, max_depth=7, n_estimators=300; total time=   0.8s\n[CV 4/5; 22/72] START criterion=gini, max_depth=7, n_estimators=300.............\n[CV 4/5; 22/72] END criterion=gini, max_depth=7, n_estimators=300; total time=   0.8s\n[CV 5/5; 22/72] START criterion=gini, max_depth=7, n_estimators=300.............\n[CV 5/5; 22/72] END criterion=gini, max_depth=7, n_estimators=300; total time=   0.9s\n[CV 1/5; 23/72] START criterion=gini, max_depth=7, n_estimators=400.............\n[CV 1/5; 23/72] END criterion=gini, max_depth=7, n_estimators=400; total time=   1.0s\n[CV 2/5; 23/72] START criterion=gini, max_depth=7, n_estimators=400.............\n[CV 2/5; 23/72] END criterion=gini, max_depth=7, n_estimators=400; total time=   1.1s\n[CV 3/5; 23/72] START criterion=gini, max_depth=7, n_estimators=400.............\n[CV 3/5; 23/72] END criterion=gini, max_depth=7, n_estimators=400; total time=   1.2s\n[CV 4/5; 23/72] START criterion=gini, max_depth=7, n_estimators=400.............\n[CV 4/5; 23/72] END criterion=gini, max_depth=7, n_estimators=400; total time=   1.0s\n[CV 5/5; 23/72] START criterion=gini, max_depth=7, n_estimators=400.............\n[CV 5/5; 23/72] END criterion=gini, max_depth=7, n_estimators=400; total time=   1.0s\n[CV 1/5; 24/72] START criterion=gini, max_depth=7, n_estimators=500.............\n[CV 1/5; 24/72] END criterion=gini, max_depth=7, n_estimators=500; total time=   1.2s\n[CV 2/5; 24/72] START criterion=gini, max_depth=7, n_estimators=500.............\n[CV 2/5; 24/72] END criterion=gini, max_depth=7, n_estimators=500; total time=   1.2s\n[CV 3/5; 24/72] START criterion=gini, max_depth=7, n_estimators=500.............\n[CV 3/5; 24/72] END criterion=gini, max_depth=7, n_estimators=500; total time=   1.2s\n[CV 4/5; 24/72] START criterion=gini, max_depth=7, n_estimators=500.............\n[CV 4/5; 24/72] END criterion=gini, max_depth=7, n_estimators=500; total time=   1.5s\n[CV 5/5; 24/72] START criterion=gini, max_depth=7, n_estimators=500.............\n[CV 5/5; 24/72] END criterion=gini, max_depth=7, n_estimators=500; total time=   1.2s\n[CV 1/5; 25/72] START criterion=gini, max_depth=11, n_estimators=100............\n[CV 1/5; 25/72] END criterion=gini, max_depth=11, n_estimators=100; total time=   0.4s\n[CV 2/5; 25/72] START criterion=gini, max_depth=11, n_estimators=100............\n[CV 2/5; 25/72] END criterion=gini, max_depth=11, n_estimators=100; total time=   0.4s\n[CV 3/5; 25/72] START criterion=gini, max_depth=11, n_estimators=100............\n[CV 3/5; 25/72] END criterion=gini, max_depth=11, n_estimators=100; total time=   0.4s\n[CV 4/5; 25/72] START criterion=gini, max_depth=11, n_estimators=100............\n[CV 4/5; 25/72] END criterion=gini, max_depth=11, n_estimators=100; total time=   0.4s\n[CV 5/5; 25/72] START criterion=gini, max_depth=11, n_estimators=100............\n[CV 5/5; 25/72] END criterion=gini, max_depth=11, n_estimators=100; total time=   0.4s\n[CV 1/5; 26/72] START criterion=gini, max_depth=11, n_estimators=200............\n[CV 1/5; 26/72] END criterion=gini, max_depth=11, n_estimators=200; total time=   0.6s\n[CV 2/5; 26/72] START criterion=gini, max_depth=11, n_estimators=200............\n[CV 2/5; 26/72] END criterion=gini, max_depth=11, n_estimators=200; total time=   0.6s\n[CV 3/5; 26/72] START criterion=gini, max_depth=11, n_estimators=200............\n[CV 3/5; 26/72] END criterion=gini, max_depth=11, n_estimators=200; total time=   0.7s\n[CV 4/5; 26/72] START criterion=gini, max_depth=11, n_estimators=200............\n[CV 4/5; 26/72] END criterion=gini, max_depth=11, n_estimators=200; total time=   0.6s\n[CV 5/5; 26/72] START criterion=gini, max_depth=11, n_estimators=200............\n[CV 5/5; 26/72] END criterion=gini, max_depth=11, n_estimators=200; total time=   0.6s\n[CV 1/5; 27/72] START criterion=gini, max_depth=11, n_estimators=250............\n[CV 1/5; 27/72] END criterion=gini, max_depth=11, n_estimators=250; total time=   0.7s\n[CV 2/5; 27/72] START criterion=gini, max_depth=11, n_estimators=250............\n[CV 2/5; 27/72] END criterion=gini, max_depth=11, n_estimators=250; total time=   0.7s\n[CV 3/5; 27/72] START criterion=gini, max_depth=11, n_estimators=250............\n[CV 3/5; 27/72] END criterion=gini, max_depth=11, n_estimators=250; total time=   0.7s\n[CV 4/5; 27/72] START criterion=gini, max_depth=11, n_estimators=250............\n[CV 4/5; 27/72] END criterion=gini, max_depth=11, n_estimators=250; total time=   0.7s\n[CV 5/5; 27/72] START criterion=gini, max_depth=11, n_estimators=250............\n[CV 5/5; 27/72] END criterion=gini, max_depth=11, n_estimators=250; total time=   0.7s\n[CV 1/5; 28/72] START criterion=gini, max_depth=11, n_estimators=300............\n[CV 1/5; 28/72] END criterion=gini, max_depth=11, n_estimators=300; total time=   0.9s\n[CV 2/5; 28/72] START criterion=gini, max_depth=11, n_estimators=300............\n[CV 2/5; 28/72] END criterion=gini, max_depth=11, n_estimators=300; total time=   0.9s\n[CV 3/5; 28/72] START criterion=gini, max_depth=11, n_estimators=300............\n[CV 3/5; 28/72] END criterion=gini, max_depth=11, n_estimators=300; total time=   0.9s\n[CV 4/5; 28/72] START criterion=gini, max_depth=11, n_estimators=300............\n[CV 4/5; 28/72] END criterion=gini, max_depth=11, n_estimators=300; total time=   0.9s\n[CV 5/5; 28/72] START criterion=gini, max_depth=11, n_estimators=300............\n[CV 5/5; 28/72] END criterion=gini, max_depth=11, n_estimators=300; total time=   0.9s\n[CV 1/5; 29/72] START criterion=gini, max_depth=11, n_estimators=400............\n[CV 1/5; 29/72] END criterion=gini, max_depth=11, n_estimators=400; total time=   1.1s\n[CV 2/5; 29/72] START criterion=gini, max_depth=11, n_estimators=400............\n[CV 2/5; 29/72] END criterion=gini, max_depth=11, n_estimators=400; total time=   1.2s\n[CV 3/5; 29/72] START criterion=gini, max_depth=11, n_estimators=400............\n[CV 3/5; 29/72] END criterion=gini, max_depth=11, n_estimators=400; total time=   1.1s\n[CV 4/5; 29/72] START criterion=gini, max_depth=11, n_estimators=400............\n[CV 4/5; 29/72] END criterion=gini, max_depth=11, n_estimators=400; total time=   1.2s\n[CV 5/5; 29/72] START criterion=gini, max_depth=11, n_estimators=400............\n[CV 5/5; 29/72] END criterion=gini, max_depth=11, n_estimators=400; total time=   1.2s\n[CV 1/5; 30/72] START criterion=gini, max_depth=11, n_estimators=500............\n[CV 1/5; 30/72] END criterion=gini, max_depth=11, n_estimators=500; total time=   1.4s\n[CV 2/5; 30/72] START criterion=gini, max_depth=11, n_estimators=500............\n[CV 2/5; 30/72] END criterion=gini, max_depth=11, n_estimators=500; total time=   1.4s\n[CV 3/5; 30/72] START criterion=gini, max_depth=11, n_estimators=500............\n[CV 3/5; 30/72] END criterion=gini, max_depth=11, n_estimators=500; total time=   1.3s\n[CV 4/5; 30/72] START criterion=gini, max_depth=11, n_estimators=500............\n[CV 4/5; 30/72] END criterion=gini, max_depth=11, n_estimators=500; total time=   1.4s\n[CV 5/5; 30/72] START criterion=gini, max_depth=11, n_estimators=500............\n[CV 5/5; 30/72] END criterion=gini, max_depth=11, n_estimators=500; total time=   1.4s\n[CV 1/5; 31/72] START criterion=gini, max_depth=15, n_estimators=100............\n[CV 1/5; 31/72] END criterion=gini, max_depth=15, n_estimators=100; total time=   0.4s\n[CV 2/5; 31/72] START criterion=gini, max_depth=15, n_estimators=100............\n[CV 2/5; 31/72] END criterion=gini, max_depth=15, n_estimators=100; total time=   0.4s\n[CV 3/5; 31/72] START criterion=gini, max_depth=15, n_estimators=100............\n[CV 3/5; 31/72] END criterion=gini, max_depth=15, n_estimators=100; total time=   0.4s\n[CV 4/5; 31/72] START criterion=gini, max_depth=15, n_estimators=100............\n[CV 4/5; 31/72] END criterion=gini, max_depth=15, n_estimators=100; total time=   0.4s\n[CV 5/5; 31/72] START criterion=gini, max_depth=15, n_estimators=100............\n[CV 5/5; 31/72] END criterion=gini, max_depth=15, n_estimators=100; total time=   0.4s\n[CV 1/5; 32/72] START criterion=gini, max_depth=15, n_estimators=200............\n[CV 1/5; 32/72] END criterion=gini, max_depth=15, n_estimators=200; total time=   0.6s\n[CV 2/5; 32/72] START criterion=gini, max_depth=15, n_estimators=200............\n[CV 2/5; 32/72] END criterion=gini, max_depth=15, n_estimators=200; total time=   0.6s\n[CV 3/5; 32/72] START criterion=gini, max_depth=15, n_estimators=200............\n[CV 3/5; 32/72] END criterion=gini, max_depth=15, n_estimators=200; total time=   0.6s\n[CV 4/5; 32/72] START criterion=gini, max_depth=15, n_estimators=200............\n[CV 4/5; 32/72] END criterion=gini, max_depth=15, n_estimators=200; total time=   0.6s\n[CV 5/5; 32/72] START criterion=gini, max_depth=15, n_estimators=200............\n[CV 5/5; 32/72] END criterion=gini, max_depth=15, n_estimators=200; total time=   0.6s\n[CV 1/5; 33/72] START criterion=gini, max_depth=15, n_estimators=250............\n[CV 1/5; 33/72] END criterion=gini, max_depth=15, n_estimators=250; total time=   0.9s\n[CV 2/5; 33/72] START criterion=gini, max_depth=15, n_estimators=250............\n[CV 2/5; 33/72] END criterion=gini, max_depth=15, n_estimators=250; total time=   0.7s\n[CV 3/5; 33/72] START criterion=gini, max_depth=15, n_estimators=250............\n[CV 3/5; 33/72] END criterion=gini, max_depth=15, n_estimators=250; total time=   0.7s\n[CV 4/5; 33/72] START criterion=gini, max_depth=15, n_estimators=250............\n[CV 4/5; 33/72] END criterion=gini, max_depth=15, n_estimators=250; total time=   0.7s\n[CV 5/5; 33/72] START criterion=gini, max_depth=15, n_estimators=250............\n[CV 5/5; 33/72] END criterion=gini, max_depth=15, n_estimators=250; total time=   0.7s\n[CV 1/5; 34/72] START criterion=gini, max_depth=15, n_estimators=300............\n[CV 1/5; 34/72] END criterion=gini, max_depth=15, n_estimators=300; total time=   0.9s\n[CV 2/5; 34/72] START criterion=gini, max_depth=15, n_estimators=300............\n[CV 2/5; 34/72] END criterion=gini, max_depth=15, n_estimators=300; total time=   0.9s\n[CV 3/5; 34/72] START criterion=gini, max_depth=15, n_estimators=300............\n[CV 3/5; 34/72] END criterion=gini, max_depth=15, n_estimators=300; total time=   1.0s\n[CV 4/5; 34/72] START criterion=gini, max_depth=15, n_estimators=300............\n[CV 4/5; 34/72] END criterion=gini, max_depth=15, n_estimators=300; total time=   0.9s\n[CV 5/5; 34/72] START criterion=gini, max_depth=15, n_estimators=300............\n[CV 5/5; 34/72] END criterion=gini, max_depth=15, n_estimators=300; total time=   0.9s\n[CV 1/5; 35/72] START criterion=gini, max_depth=15, n_estimators=400............\n[CV 1/5; 35/72] END criterion=gini, max_depth=15, n_estimators=400; total time=   1.2s\n[CV 2/5; 35/72] START criterion=gini, max_depth=15, n_estimators=400............\n[CV 2/5; 35/72] END criterion=gini, max_depth=15, n_estimators=400; total time=   1.2s\n[CV 3/5; 35/72] START criterion=gini, max_depth=15, n_estimators=400............\n[CV 3/5; 35/72] END criterion=gini, max_depth=15, n_estimators=400; total time=   1.2s\n[CV 4/5; 35/72] START criterion=gini, max_depth=15, n_estimators=400............\n[CV 4/5; 35/72] END criterion=gini, max_depth=15, n_estimators=400; total time=   1.2s\n[CV 5/5; 35/72] START criterion=gini, max_depth=15, n_estimators=400............\n[CV 5/5; 35/72] END criterion=gini, max_depth=15, n_estimators=400; total time=   1.2s\n[CV 1/5; 36/72] START criterion=gini, max_depth=15, n_estimators=500............\n[CV 1/5; 36/72] END criterion=gini, max_depth=15, n_estimators=500; total time=   1.4s\n[CV 2/5; 36/72] START criterion=gini, max_depth=15, n_estimators=500............\n[CV 2/5; 36/72] END criterion=gini, max_depth=15, n_estimators=500; total time=   1.5s\n[CV 3/5; 36/72] START criterion=gini, max_depth=15, n_estimators=500............\n[CV 3/5; 36/72] END criterion=gini, max_depth=15, n_estimators=500; total time=   1.4s\n[CV 4/5; 36/72] START criterion=gini, max_depth=15, n_estimators=500............\n[CV 4/5; 36/72] END criterion=gini, max_depth=15, n_estimators=500; total time=   1.4s\n[CV 5/5; 36/72] START criterion=gini, max_depth=15, n_estimators=500............\n[CV 5/5; 36/72] END criterion=gini, max_depth=15, n_estimators=500; total time=   1.4s\n[CV 1/5; 37/72] START criterion=entropy, max_depth=1, n_estimators=100..........\n[CV 1/5; 37/72] END criterion=entropy, max_depth=1, n_estimators=100; total time=   0.3s\n[CV 2/5; 37/72] START criterion=entropy, max_depth=1, n_estimators=100..........\n[CV 2/5; 37/72] END criterion=entropy, max_depth=1, n_estimators=100; total time=   0.3s\n[CV 3/5; 37/72] START criterion=entropy, max_depth=1, n_estimators=100..........\n[CV 3/5; 37/72] END criterion=entropy, max_depth=1, n_estimators=100; total time=   0.4s\n[CV 4/5; 37/72] START criterion=entropy, max_depth=1, n_estimators=100..........\n[CV 4/5; 37/72] END criterion=entropy, max_depth=1, n_estimators=100; total time=   0.4s\n[CV 5/5; 37/72] START criterion=entropy, max_depth=1, n_estimators=100..........\n[CV 5/5; 37/72] END criterion=entropy, max_depth=1, n_estimators=100; total time=   0.3s\n[CV 1/5; 38/72] START criterion=entropy, max_depth=1, n_estimators=200..........\n[CV 1/5; 38/72] END criterion=entropy, max_depth=1, n_estimators=200; total time=   0.5s\n[CV 2/5; 38/72] START criterion=entropy, max_depth=1, n_estimators=200..........\n[CV 2/5; 38/72] END criterion=entropy, max_depth=1, n_estimators=200; total time=   0.5s\n[CV 3/5; 38/72] START criterion=entropy, max_depth=1, n_estimators=200..........\n[CV 3/5; 38/72] END criterion=entropy, max_depth=1, n_estimators=200; total time=   0.5s\n[CV 4/5; 38/72] START criterion=entropy, max_depth=1, n_estimators=200..........\n[CV 4/5; 38/72] END criterion=entropy, max_depth=1, n_estimators=200; total time=   0.5s\n[CV 5/5; 38/72] START criterion=entropy, max_depth=1, n_estimators=200..........\n[CV 5/5; 38/72] END criterion=entropy, max_depth=1, n_estimators=200; total time=   0.5s\n[CV 1/5; 39/72] START criterion=entropy, max_depth=1, n_estimators=250..........\n[CV 1/5; 39/72] END criterion=entropy, max_depth=1, n_estimators=250; total time=   0.6s\n[CV 2/5; 39/72] START criterion=entropy, max_depth=1, n_estimators=250..........\n[CV 2/5; 39/72] END criterion=entropy, max_depth=1, n_estimators=250; total time=   0.6s\n[CV 3/5; 39/72] START criterion=entropy, max_depth=1, n_estimators=250..........\n[CV 3/5; 39/72] END criterion=entropy, max_depth=1, n_estimators=250; total time=   0.6s\n[CV 4/5; 39/72] START criterion=entropy, max_depth=1, n_estimators=250..........\n[CV 4/5; 39/72] END criterion=entropy, max_depth=1, n_estimators=250; total time=   0.6s\n[CV 5/5; 39/72] START criterion=entropy, max_depth=1, n_estimators=250..........\n[CV 5/5; 39/72] END criterion=entropy, max_depth=1, n_estimators=250; total time=   0.6s\n[CV 1/5; 40/72] START criterion=entropy, max_depth=1, n_estimators=300..........\n[CV 1/5; 40/72] END criterion=entropy, max_depth=1, n_estimators=300; total time=   0.7s\n[CV 2/5; 40/72] START criterion=entropy, max_depth=1, n_estimators=300..........\n[CV 2/5; 40/72] END criterion=entropy, max_depth=1, n_estimators=300; total time=   0.8s\n[CV 3/5; 40/72] START criterion=entropy, max_depth=1, n_estimators=300..........\n[CV 3/5; 40/72] END criterion=entropy, max_depth=1, n_estimators=300; total time=   0.8s\n[CV 4/5; 40/72] START criterion=entropy, max_depth=1, n_estimators=300..........\n[CV 4/5; 40/72] END criterion=entropy, max_depth=1, n_estimators=300; total time=   0.8s\n[CV 5/5; 40/72] START criterion=entropy, max_depth=1, n_estimators=300..........\n[CV 5/5; 40/72] END criterion=entropy, max_depth=1, n_estimators=300; total time=   0.8s\n[CV 1/5; 41/72] START criterion=entropy, max_depth=1, n_estimators=400..........\n[CV 1/5; 41/72] END criterion=entropy, max_depth=1, n_estimators=400; total time=   1.1s\n[CV 2/5; 41/72] START criterion=entropy, max_depth=1, n_estimators=400..........\n[CV 2/5; 41/72] END criterion=entropy, max_depth=1, n_estimators=400; total time=   0.9s\n[CV 3/5; 41/72] START criterion=entropy, max_depth=1, n_estimators=400..........\n[CV 3/5; 41/72] END criterion=entropy, max_depth=1, n_estimators=400; total time=   0.9s\n[CV 4/5; 41/72] START criterion=entropy, max_depth=1, n_estimators=400..........\n[CV 4/5; 41/72] END criterion=entropy, max_depth=1, n_estimators=400; total time=   1.0s\n[CV 5/5; 41/72] START criterion=entropy, max_depth=1, n_estimators=400..........\n[CV 5/5; 41/72] END criterion=entropy, max_depth=1, n_estimators=400; total time=   0.9s\n[CV 1/5; 42/72] START criterion=entropy, max_depth=1, n_estimators=500..........\n[CV 1/5; 42/72] END criterion=entropy, max_depth=1, n_estimators=500; total time=   1.1s\n[CV 2/5; 42/72] START criterion=entropy, max_depth=1, n_estimators=500..........\n[CV 2/5; 42/72] END criterion=entropy, max_depth=1, n_estimators=500; total time=   1.1s\n[CV 3/5; 42/72] START criterion=entropy, max_depth=1, n_estimators=500..........\n[CV 3/5; 42/72] END criterion=entropy, max_depth=1, n_estimators=500; total time=   1.1s\n[CV 4/5; 42/72] START criterion=entropy, max_depth=1, n_estimators=500..........\n[CV 4/5; 42/72] END criterion=entropy, max_depth=1, n_estimators=500; total time=   1.1s\n[CV 5/5; 42/72] START criterion=entropy, max_depth=1, n_estimators=500..........\n[CV 5/5; 42/72] END criterion=entropy, max_depth=1, n_estimators=500; total time=   1.1s\n[CV 1/5; 43/72] START criterion=entropy, max_depth=2, n_estimators=100..........\n[CV 1/5; 43/72] END criterion=entropy, max_depth=2, n_estimators=100; total time=   0.4s\n[CV 2/5; 43/72] START criterion=entropy, max_depth=2, n_estimators=100..........\n[CV 2/5; 43/72] END criterion=entropy, max_depth=2, n_estimators=100; total time=   0.4s\n[CV 3/5; 43/72] START criterion=entropy, max_depth=2, n_estimators=100..........\n[CV 3/5; 43/72] END criterion=entropy, max_depth=2, n_estimators=100; total time=   0.4s\n[CV 4/5; 43/72] START criterion=entropy, max_depth=2, n_estimators=100..........\n[CV 4/5; 43/72] END criterion=entropy, max_depth=2, n_estimators=100; total time=   0.3s\n[CV 5/5; 43/72] START criterion=entropy, max_depth=2, n_estimators=100..........\n[CV 5/5; 43/72] END criterion=entropy, max_depth=2, n_estimators=100; total time=   0.4s\n[CV 1/5; 44/72] START criterion=entropy, max_depth=2, n_estimators=200..........\n[CV 1/5; 44/72] END criterion=entropy, max_depth=2, n_estimators=200; total time=   0.5s\n[CV 2/5; 44/72] START criterion=entropy, max_depth=2, n_estimators=200..........\n[CV 2/5; 44/72] END criterion=entropy, max_depth=2, n_estimators=200; total time=   0.5s\n[CV 3/5; 44/72] START criterion=entropy, max_depth=2, n_estimators=200..........\n[CV 3/5; 44/72] END criterion=entropy, max_depth=2, n_estimators=200; total time=   0.6s\n[CV 4/5; 44/72] START criterion=entropy, max_depth=2, n_estimators=200..........\n[CV 4/5; 44/72] END criterion=entropy, max_depth=2, n_estimators=200; total time=   0.5s\n[CV 5/5; 44/72] START criterion=entropy, max_depth=2, n_estimators=200..........\n[CV 5/5; 44/72] END criterion=entropy, max_depth=2, n_estimators=200; total time=   0.5s\n[CV 1/5; 45/72] START criterion=entropy, max_depth=2, n_estimators=250..........\n[CV 1/5; 45/72] END criterion=entropy, max_depth=2, n_estimators=250; total time=   0.6s\n[CV 2/5; 45/72] START criterion=entropy, max_depth=2, n_estimators=250..........\n[CV 2/5; 45/72] END criterion=entropy, max_depth=2, n_estimators=250; total time=   0.6s\n[CV 3/5; 45/72] START criterion=entropy, max_depth=2, n_estimators=250..........\n[CV 3/5; 45/72] END criterion=entropy, max_depth=2, n_estimators=250; total time=   0.6s\n[CV 4/5; 45/72] START criterion=entropy, max_depth=2, n_estimators=250..........\n[CV 4/5; 45/72] END criterion=entropy, max_depth=2, n_estimators=250; total time=   0.6s\n[CV 5/5; 45/72] START criterion=entropy, max_depth=2, n_estimators=250..........\n[CV 5/5; 45/72] END criterion=entropy, max_depth=2, n_estimators=250; total time=   0.6s\n[CV 1/5; 46/72] START criterion=entropy, max_depth=2, n_estimators=300..........\n[CV 1/5; 46/72] END criterion=entropy, max_depth=2, n_estimators=300; total time=   0.8s\n[CV 2/5; 46/72] START criterion=entropy, max_depth=2, n_estimators=300..........\n[CV 2/5; 46/72] END criterion=entropy, max_depth=2, n_estimators=300; total time=   0.8s\n[CV 3/5; 46/72] START criterion=entropy, max_depth=2, n_estimators=300..........\n[CV 3/5; 46/72] END criterion=entropy, max_depth=2, n_estimators=300; total time=   0.8s\n[CV 4/5; 46/72] START criterion=entropy, max_depth=2, n_estimators=300..........\n[CV 4/5; 46/72] END criterion=entropy, max_depth=2, n_estimators=300; total time=   0.8s\n[CV 5/5; 46/72] START criterion=entropy, max_depth=2, n_estimators=300..........\n[CV 5/5; 46/72] END criterion=entropy, max_depth=2, n_estimators=300; total time=   0.8s\n[CV 1/5; 47/72] START criterion=entropy, max_depth=2, n_estimators=400..........\n[CV 1/5; 47/72] END criterion=entropy, max_depth=2, n_estimators=400; total time=   1.0s\n[CV 2/5; 47/72] START criterion=entropy, max_depth=2, n_estimators=400..........\n[CV 2/5; 47/72] END criterion=entropy, max_depth=2, n_estimators=400; total time=   0.9s\n[CV 3/5; 47/72] START criterion=entropy, max_depth=2, n_estimators=400..........\n[CV 3/5; 47/72] END criterion=entropy, max_depth=2, n_estimators=400; total time=   0.9s\n[CV 4/5; 47/72] START criterion=entropy, max_depth=2, n_estimators=400..........\n[CV 4/5; 47/72] END criterion=entropy, max_depth=2, n_estimators=400; total time=   1.0s\n[CV 5/5; 47/72] START criterion=entropy, max_depth=2, n_estimators=400..........\n[CV 5/5; 47/72] END criterion=entropy, max_depth=2, n_estimators=400; total time=   1.0s\n[CV 1/5; 48/72] START criterion=entropy, max_depth=2, n_estimators=500..........\n[CV 1/5; 48/72] END criterion=entropy, max_depth=2, n_estimators=500; total time=   1.2s\n[CV 2/5; 48/72] START criterion=entropy, max_depth=2, n_estimators=500..........\n[CV 2/5; 48/72] END criterion=entropy, max_depth=2, n_estimators=500; total time=   1.2s\n[CV 3/5; 48/72] START criterion=entropy, max_depth=2, n_estimators=500..........\n[CV 3/5; 48/72] END criterion=entropy, max_depth=2, n_estimators=500; total time=   1.2s\n[CV 4/5; 48/72] START criterion=entropy, max_depth=2, n_estimators=500..........\n[CV 4/5; 48/72] END criterion=entropy, max_depth=2, n_estimators=500; total time=   1.2s\n[CV 5/5; 48/72] START criterion=entropy, max_depth=2, n_estimators=500..........\n[CV 5/5; 48/72] END criterion=entropy, max_depth=2, n_estimators=500; total time=   1.2s\n[CV 1/5; 49/72] START criterion=entropy, max_depth=5, n_estimators=100..........\n[CV 1/5; 49/72] END criterion=entropy, max_depth=5, n_estimators=100; total time=   0.4s\n[CV 2/5; 49/72] START criterion=entropy, max_depth=5, n_estimators=100..........\n[CV 2/5; 49/72] END criterion=entropy, max_depth=5, n_estimators=100; total time=   0.4s\n[CV 3/5; 49/72] START criterion=entropy, max_depth=5, n_estimators=100..........\n[CV 3/5; 49/72] END criterion=entropy, max_depth=5, n_estimators=100; total time=   0.4s\n[CV 4/5; 49/72] START criterion=entropy, max_depth=5, n_estimators=100..........\n[CV 4/5; 49/72] END criterion=entropy, max_depth=5, n_estimators=100; total time=   0.5s\n[CV 5/5; 49/72] START criterion=entropy, max_depth=5, n_estimators=100..........\n[CV 5/5; 49/72] END criterion=entropy, max_depth=5, n_estimators=100; total time=   0.4s\n[CV 1/5; 50/72] START criterion=entropy, max_depth=5, n_estimators=200..........\n[CV 1/5; 50/72] END criterion=entropy, max_depth=5, n_estimators=200; total time=   0.6s\n[CV 2/5; 50/72] START criterion=entropy, max_depth=5, n_estimators=200..........\n[CV 2/5; 50/72] END criterion=entropy, max_depth=5, n_estimators=200; total time=   0.6s\n[CV 3/5; 50/72] START criterion=entropy, max_depth=5, n_estimators=200..........\n[CV 3/5; 50/72] END criterion=entropy, max_depth=5, n_estimators=200; total time=   0.6s\n[CV 4/5; 50/72] START criterion=entropy, max_depth=5, n_estimators=200..........\n[CV 4/5; 50/72] END criterion=entropy, max_depth=5, n_estimators=200; total time=   0.6s\n[CV 5/5; 50/72] START criterion=entropy, max_depth=5, n_estimators=200..........\n[CV 5/5; 50/72] END criterion=entropy, max_depth=5, n_estimators=200; total time=   0.6s\n[CV 1/5; 51/72] START criterion=entropy, max_depth=5, n_estimators=250..........\n[CV 1/5; 51/72] END criterion=entropy, max_depth=5, n_estimators=250; total time=   0.7s\n[CV 2/5; 51/72] START criterion=entropy, max_depth=5, n_estimators=250..........\n[CV 2/5; 51/72] END criterion=entropy, max_depth=5, n_estimators=250; total time=   0.7s\n[CV 3/5; 51/72] START criterion=entropy, max_depth=5, n_estimators=250..........\n[CV 3/5; 51/72] END criterion=entropy, max_depth=5, n_estimators=250; total time=   0.7s\n[CV 4/5; 51/72] START criterion=entropy, max_depth=5, n_estimators=250..........\n[CV 4/5; 51/72] END criterion=entropy, max_depth=5, n_estimators=250; total time=   0.7s\n[CV 5/5; 51/72] START criterion=entropy, max_depth=5, n_estimators=250..........\n[CV 5/5; 51/72] END criterion=entropy, max_depth=5, n_estimators=250; total time=   0.7s\n[CV 1/5; 52/72] START criterion=entropy, max_depth=5, n_estimators=300..........\n[CV 1/5; 52/72] END criterion=entropy, max_depth=5, n_estimators=300; total time=   1.0s\n[CV 2/5; 52/72] START criterion=entropy, max_depth=5, n_estimators=300..........\n[CV 2/5; 52/72] END criterion=entropy, max_depth=5, n_estimators=300; total time=   1.0s\n[CV 3/5; 52/72] START criterion=entropy, max_depth=5, n_estimators=300..........\n[CV 3/5; 52/72] END criterion=entropy, max_depth=5, n_estimators=300; total time=   1.1s\n[CV 4/5; 52/72] START criterion=entropy, max_depth=5, n_estimators=300..........\n[CV 4/5; 52/72] END criterion=entropy, max_depth=5, n_estimators=300; total time=   1.0s\n[CV 5/5; 52/72] START criterion=entropy, max_depth=5, n_estimators=300..........\n[CV 5/5; 52/72] END criterion=entropy, max_depth=5, n_estimators=300; total time=   0.9s\n[CV 1/5; 53/72] START criterion=entropy, max_depth=5, n_estimators=400..........\n[CV 1/5; 53/72] END criterion=entropy, max_depth=5, n_estimators=400; total time=   1.1s\n[CV 2/5; 53/72] START criterion=entropy, max_depth=5, n_estimators=400..........\n[CV 2/5; 53/72] END criterion=entropy, max_depth=5, n_estimators=400; total time=   1.1s\n[CV 3/5; 53/72] START criterion=entropy, max_depth=5, n_estimators=400..........\n[CV 3/5; 53/72] END criterion=entropy, max_depth=5, n_estimators=400; total time=   1.2s\n[CV 4/5; 53/72] START criterion=entropy, max_depth=5, n_estimators=400..........\n[CV 4/5; 53/72] END criterion=entropy, max_depth=5, n_estimators=400; total time=   1.2s\n[CV 5/5; 53/72] START criterion=entropy, max_depth=5, n_estimators=400..........\n[CV 5/5; 53/72] END criterion=entropy, max_depth=5, n_estimators=400; total time=   1.1s\n[CV 1/5; 54/72] START criterion=entropy, max_depth=5, n_estimators=500..........\n[CV 1/5; 54/72] END criterion=entropy, max_depth=5, n_estimators=500; total time=   1.3s\n[CV 2/5; 54/72] START criterion=entropy, max_depth=5, n_estimators=500..........\n[CV 2/5; 54/72] END criterion=entropy, max_depth=5, n_estimators=500; total time=   1.4s\n[CV 3/5; 54/72] START criterion=entropy, max_depth=5, n_estimators=500..........\n[CV 3/5; 54/72] END criterion=entropy, max_depth=5, n_estimators=500; total time=   1.3s\n[CV 4/5; 54/72] START criterion=entropy, max_depth=5, n_estimators=500..........\n[CV 4/5; 54/72] END criterion=entropy, max_depth=5, n_estimators=500; total time=   1.3s\n[CV 5/5; 54/72] START criterion=entropy, max_depth=5, n_estimators=500..........\n[CV 5/5; 54/72] END criterion=entropy, max_depth=5, n_estimators=500; total time=   1.3s\n[CV 1/5; 55/72] START criterion=entropy, max_depth=7, n_estimators=100..........\n[CV 1/5; 55/72] END criterion=entropy, max_depth=7, n_estimators=100; total time=   0.4s\n[CV 2/5; 55/72] START criterion=entropy, max_depth=7, n_estimators=100..........\n[CV 2/5; 55/72] END criterion=entropy, max_depth=7, n_estimators=100; total time=   0.4s\n[CV 3/5; 55/72] START criterion=entropy, max_depth=7, n_estimators=100..........\n[CV 3/5; 55/72] END criterion=entropy, max_depth=7, n_estimators=100; total time=   0.4s\n[CV 4/5; 55/72] START criterion=entropy, max_depth=7, n_estimators=100..........\n[CV 4/5; 55/72] END criterion=entropy, max_depth=7, n_estimators=100; total time=   0.4s\n[CV 5/5; 55/72] START criterion=entropy, max_depth=7, n_estimators=100..........\n[CV 5/5; 55/72] END criterion=entropy, max_depth=7, n_estimators=100; total time=   0.4s\n[CV 1/5; 56/72] START criterion=entropy, max_depth=7, n_estimators=200..........\n[CV 1/5; 56/72] END criterion=entropy, max_depth=7, n_estimators=200; total time=   0.6s\n[CV 2/5; 56/72] START criterion=entropy, max_depth=7, n_estimators=200..........\n[CV 2/5; 56/72] END criterion=entropy, max_depth=7, n_estimators=200; total time=   0.7s\n[CV 3/5; 56/72] START criterion=entropy, max_depth=7, n_estimators=200..........\n[CV 3/5; 56/72] END criterion=entropy, max_depth=7, n_estimators=200; total time=   0.6s\n[CV 4/5; 56/72] START criterion=entropy, max_depth=7, n_estimators=200..........\n[CV 4/5; 56/72] END criterion=entropy, max_depth=7, n_estimators=200; total time=   0.7s\n[CV 5/5; 56/72] START criterion=entropy, max_depth=7, n_estimators=200..........\n[CV 5/5; 56/72] END criterion=entropy, max_depth=7, n_estimators=200; total time=   0.6s\n[CV 1/5; 57/72] START criterion=entropy, max_depth=7, n_estimators=250..........\n[CV 1/5; 57/72] END criterion=entropy, max_depth=7, n_estimators=250; total time=   0.7s\n[CV 2/5; 57/72] START criterion=entropy, max_depth=7, n_estimators=250..........\n[CV 2/5; 57/72] END criterion=entropy, max_depth=7, n_estimators=250; total time=   0.8s\n[CV 3/5; 57/72] START criterion=entropy, max_depth=7, n_estimators=250..........\n[CV 3/5; 57/72] END criterion=entropy, max_depth=7, n_estimators=250; total time=   1.0s\n[CV 4/5; 57/72] START criterion=entropy, max_depth=7, n_estimators=250..........\n[CV 4/5; 57/72] END criterion=entropy, max_depth=7, n_estimators=250; total time=   0.8s\n[CV 5/5; 57/72] START criterion=entropy, max_depth=7, n_estimators=250..........\n[CV 5/5; 57/72] END criterion=entropy, max_depth=7, n_estimators=250; total time=   0.7s\n[CV 1/5; 58/72] START criterion=entropy, max_depth=7, n_estimators=300..........\n[CV 1/5; 58/72] END criterion=entropy, max_depth=7, n_estimators=300; total time=   1.0s\n[CV 2/5; 58/72] START criterion=entropy, max_depth=7, n_estimators=300..........\n[CV 2/5; 58/72] END criterion=entropy, max_depth=7, n_estimators=300; total time=   0.9s\n[CV 3/5; 58/72] START criterion=entropy, max_depth=7, n_estimators=300..........\n[CV 3/5; 58/72] END criterion=entropy, max_depth=7, n_estimators=300; total time=   1.0s\n[CV 4/5; 58/72] START criterion=entropy, max_depth=7, n_estimators=300..........\n[CV 4/5; 58/72] END criterion=entropy, max_depth=7, n_estimators=300; total time=   0.9s\n[CV 5/5; 58/72] START criterion=entropy, max_depth=7, n_estimators=300..........\n[CV 5/5; 58/72] END criterion=entropy, max_depth=7, n_estimators=300; total time=   1.0s\n[CV 1/5; 59/72] START criterion=entropy, max_depth=7, n_estimators=400..........\n[CV 1/5; 59/72] END criterion=entropy, max_depth=7, n_estimators=400; total time=   1.2s\n[CV 2/5; 59/72] START criterion=entropy, max_depth=7, n_estimators=400..........\n[CV 2/5; 59/72] END criterion=entropy, max_depth=7, n_estimators=400; total time=   1.2s\n[CV 3/5; 59/72] START criterion=entropy, max_depth=7, n_estimators=400..........\n[CV 3/5; 59/72] END criterion=entropy, max_depth=7, n_estimators=400; total time=   1.2s\n[CV 4/5; 59/72] START criterion=entropy, max_depth=7, n_estimators=400..........\n[CV 4/5; 59/72] END criterion=entropy, max_depth=7, n_estimators=400; total time=   1.3s\n[CV 5/5; 59/72] START criterion=entropy, max_depth=7, n_estimators=400..........\n[CV 5/5; 59/72] END criterion=entropy, max_depth=7, n_estimators=400; total time=   1.3s\n[CV 1/5; 60/72] START criterion=entropy, max_depth=7, n_estimators=500..........\n[CV 1/5; 60/72] END criterion=entropy, max_depth=7, n_estimators=500; total time=   1.5s\n[CV 2/5; 60/72] START criterion=entropy, max_depth=7, n_estimators=500..........\n[CV 2/5; 60/72] END criterion=entropy, max_depth=7, n_estimators=500; total time=   1.5s\n[CV 3/5; 60/72] START criterion=entropy, max_depth=7, n_estimators=500..........\n[CV 3/5; 60/72] END criterion=entropy, max_depth=7, n_estimators=500; total time=   1.5s\n[CV 4/5; 60/72] START criterion=entropy, max_depth=7, n_estimators=500..........\n[CV 4/5; 60/72] END criterion=entropy, max_depth=7, n_estimators=500; total time=   1.5s\n[CV 5/5; 60/72] START criterion=entropy, max_depth=7, n_estimators=500..........\n[CV 5/5; 60/72] END criterion=entropy, max_depth=7, n_estimators=500; total time=   1.5s\n[CV 1/5; 61/72] START criterion=entropy, max_depth=11, n_estimators=100.........\n[CV 1/5; 61/72] END criterion=entropy, max_depth=11, n_estimators=100; total time=   0.5s\n[CV 2/5; 61/72] START criterion=entropy, max_depth=11, n_estimators=100.........\n[CV 2/5; 61/72] END criterion=entropy, max_depth=11, n_estimators=100; total time=   0.4s\n[CV 3/5; 61/72] START criterion=entropy, max_depth=11, n_estimators=100.........\n[CV 3/5; 61/72] END criterion=entropy, max_depth=11, n_estimators=100; total time=   0.4s\n[CV 4/5; 61/72] START criterion=entropy, max_depth=11, n_estimators=100.........\n[CV 4/5; 61/72] END criterion=entropy, max_depth=11, n_estimators=100; total time=   0.4s\n[CV 5/5; 61/72] START criterion=entropy, max_depth=11, n_estimators=100.........\n[CV 5/5; 61/72] END criterion=entropy, max_depth=11, n_estimators=100; total time=   0.4s\n[CV 1/5; 62/72] START criterion=entropy, max_depth=11, n_estimators=200.........\n[CV 1/5; 62/72] END criterion=entropy, max_depth=11, n_estimators=200; total time=   0.8s\n[CV 2/5; 62/72] START criterion=entropy, max_depth=11, n_estimators=200.........\n[CV 2/5; 62/72] END criterion=entropy, max_depth=11, n_estimators=200; total time=   0.8s\n[CV 3/5; 62/72] START criterion=entropy, max_depth=11, n_estimators=200.........\n[CV 3/5; 62/72] END criterion=entropy, max_depth=11, n_estimators=200; total time=   0.7s\n[CV 4/5; 62/72] START criterion=entropy, max_depth=11, n_estimators=200.........\n[CV 4/5; 62/72] END criterion=entropy, max_depth=11, n_estimators=200; total time=   0.7s\n[CV 5/5; 62/72] START criterion=entropy, max_depth=11, n_estimators=200.........\n[CV 5/5; 62/72] END criterion=entropy, max_depth=11, n_estimators=200; total time=   0.7s\n[CV 1/5; 63/72] START criterion=entropy, max_depth=11, n_estimators=250.........\n[CV 1/5; 63/72] END criterion=entropy, max_depth=11, n_estimators=250; total time=   0.8s\n[CV 2/5; 63/72] START criterion=entropy, max_depth=11, n_estimators=250.........\n[CV 2/5; 63/72] END criterion=entropy, max_depth=11, n_estimators=250; total time=   0.8s\n[CV 3/5; 63/72] START criterion=entropy, max_depth=11, n_estimators=250.........\n[CV 3/5; 63/72] END criterion=entropy, max_depth=11, n_estimators=250; total time=   0.9s\n[CV 4/5; 63/72] START criterion=entropy, max_depth=11, n_estimators=250.........\n[CV 4/5; 63/72] END criterion=entropy, max_depth=11, n_estimators=250; total time=   0.8s\n[CV 5/5; 63/72] START criterion=entropy, max_depth=11, n_estimators=250.........\n[CV 5/5; 63/72] END criterion=entropy, max_depth=11, n_estimators=250; total time=   0.9s\n[CV 1/5; 64/72] START criterion=entropy, max_depth=11, n_estimators=300.........\n[CV 1/5; 64/72] END criterion=entropy, max_depth=11, n_estimators=300; total time=   1.1s\n[CV 2/5; 64/72] START criterion=entropy, max_depth=11, n_estimators=300.........\n[CV 2/5; 64/72] END criterion=entropy, max_depth=11, n_estimators=300; total time=   1.2s\n[CV 3/5; 64/72] START criterion=entropy, max_depth=11, n_estimators=300.........\n[CV 3/5; 64/72] END criterion=entropy, max_depth=11, n_estimators=300; total time=   1.1s\n[CV 4/5; 64/72] START criterion=entropy, max_depth=11, n_estimators=300.........\n[CV 4/5; 64/72] END criterion=entropy, max_depth=11, n_estimators=300; total time=   1.1s\n[CV 5/5; 64/72] START criterion=entropy, max_depth=11, n_estimators=300.........\n[CV 5/5; 64/72] END criterion=entropy, max_depth=11, n_estimators=300; total time=   1.1s\n[CV 1/5; 65/72] START criterion=entropy, max_depth=11, n_estimators=400.........\n[CV 1/5; 65/72] END criterion=entropy, max_depth=11, n_estimators=400; total time=   1.5s\n[CV 2/5; 65/72] START criterion=entropy, max_depth=11, n_estimators=400.........\n[CV 2/5; 65/72] END criterion=entropy, max_depth=11, n_estimators=400; total time=   1.5s\n[CV 3/5; 65/72] START criterion=entropy, max_depth=11, n_estimators=400.........\n[CV 3/5; 65/72] END criterion=entropy, max_depth=11, n_estimators=400; total time=   1.4s\n[CV 4/5; 65/72] START criterion=entropy, max_depth=11, n_estimators=400.........\n[CV 4/5; 65/72] END criterion=entropy, max_depth=11, n_estimators=400; total time=   1.4s\n[CV 5/5; 65/72] START criterion=entropy, max_depth=11, n_estimators=400.........\n[CV 5/5; 65/72] END criterion=entropy, max_depth=11, n_estimators=400; total time=   1.4s\n[CV 1/5; 66/72] START criterion=entropy, max_depth=11, n_estimators=500.........\n[CV 1/5; 66/72] END criterion=entropy, max_depth=11, n_estimators=500; total time=   1.7s\n[CV 2/5; 66/72] START criterion=entropy, max_depth=11, n_estimators=500.........\n[CV 2/5; 66/72] END criterion=entropy, max_depth=11, n_estimators=500; total time=   1.7s\n[CV 3/5; 66/72] START criterion=entropy, max_depth=11, n_estimators=500.........\n[CV 3/5; 66/72] END criterion=entropy, max_depth=11, n_estimators=500; total time=   1.7s\n[CV 4/5; 66/72] START criterion=entropy, max_depth=11, n_estimators=500.........\n[CV 4/5; 66/72] END criterion=entropy, max_depth=11, n_estimators=500; total time=   1.7s\n[CV 5/5; 66/72] START criterion=entropy, max_depth=11, n_estimators=500.........\n[CV 5/5; 66/72] END criterion=entropy, max_depth=11, n_estimators=500; total time=   1.8s\n[CV 1/5; 67/72] START criterion=entropy, max_depth=15, n_estimators=100.........\n[CV 1/5; 67/72] END criterion=entropy, max_depth=15, n_estimators=100; total time=   0.5s\n[CV 2/5; 67/72] START criterion=entropy, max_depth=15, n_estimators=100.........\n[CV 2/5; 67/72] END criterion=entropy, max_depth=15, n_estimators=100; total time=   0.5s\n[CV 3/5; 67/72] START criterion=entropy, max_depth=15, n_estimators=100.........\n[CV 3/5; 67/72] END criterion=entropy, max_depth=15, n_estimators=100; total time=   0.5s\n[CV 4/5; 67/72] START criterion=entropy, max_depth=15, n_estimators=100.........\n[CV 4/5; 67/72] END criterion=entropy, max_depth=15, n_estimators=100; total time=   0.5s\n[CV 5/5; 67/72] START criterion=entropy, max_depth=15, n_estimators=100.........\n[CV 5/5; 67/72] END criterion=entropy, max_depth=15, n_estimators=100; total time=   0.5s\n[CV 1/5; 68/72] START criterion=entropy, max_depth=15, n_estimators=200.........\n[CV 1/5; 68/72] END criterion=entropy, max_depth=15, n_estimators=200; total time=   0.7s\n[CV 2/5; 68/72] START criterion=entropy, max_depth=15, n_estimators=200.........\n[CV 2/5; 68/72] END criterion=entropy, max_depth=15, n_estimators=200; total time=   0.8s\n[CV 3/5; 68/72] START criterion=entropy, max_depth=15, n_estimators=200.........\n[CV 3/5; 68/72] END criterion=entropy, max_depth=15, n_estimators=200; total time=   0.7s\n[CV 4/5; 68/72] START criterion=entropy, max_depth=15, n_estimators=200.........\n[CV 4/5; 68/72] END criterion=entropy, max_depth=15, n_estimators=200; total time=   0.7s\n[CV 5/5; 68/72] START criterion=entropy, max_depth=15, n_estimators=200.........\n[CV 5/5; 68/72] END criterion=entropy, max_depth=15, n_estimators=200; total time=   0.8s\n[CV 1/5; 69/72] START criterion=entropy, max_depth=15, n_estimators=250.........\n[CV 1/5; 69/72] END criterion=entropy, max_depth=15, n_estimators=250; total time=   0.9s\n[CV 2/5; 69/72] START criterion=entropy, max_depth=15, n_estimators=250.........\n[CV 2/5; 69/72] END criterion=entropy, max_depth=15, n_estimators=250; total time=   0.9s\n[CV 3/5; 69/72] START criterion=entropy, max_depth=15, n_estimators=250.........\n[CV 3/5; 69/72] END criterion=entropy, max_depth=15, n_estimators=250; total time=   0.9s\n[CV 4/5; 69/72] START criterion=entropy, max_depth=15, n_estimators=250.........\n[CV 4/5; 69/72] END criterion=entropy, max_depth=15, n_estimators=250; total time=   0.9s\n[CV 5/5; 69/72] START criterion=entropy, max_depth=15, n_estimators=250.........\n[CV 5/5; 69/72] END criterion=entropy, max_depth=15, n_estimators=250; total time=   0.9s\n[CV 1/5; 70/72] START criterion=entropy, max_depth=15, n_estimators=300.........\n[CV 1/5; 70/72] END criterion=entropy, max_depth=15, n_estimators=300; total time=   1.1s\n[CV 2/5; 70/72] START criterion=entropy, max_depth=15, n_estimators=300.........\n[CV 2/5; 70/72] END criterion=entropy, max_depth=15, n_estimators=300; total time=   1.3s\n[CV 3/5; 70/72] START criterion=entropy, max_depth=15, n_estimators=300.........\n[CV 3/5; 70/72] END criterion=entropy, max_depth=15, n_estimators=300; total time=   1.2s\n[CV 4/5; 70/72] START criterion=entropy, max_depth=15, n_estimators=300.........\n[CV 4/5; 70/72] END criterion=entropy, max_depth=15, n_estimators=300; total time=   1.1s\n[CV 5/5; 70/72] START criterion=entropy, max_depth=15, n_estimators=300.........\n[CV 5/5; 70/72] END criterion=entropy, max_depth=15, n_estimators=300; total time=   1.1s\n[CV 1/5; 71/72] START criterion=entropy, max_depth=15, n_estimators=400.........\n[CV 1/5; 71/72] END criterion=entropy, max_depth=15, n_estimators=400; total time=   1.4s\n[CV 2/5; 71/72] START criterion=entropy, max_depth=15, n_estimators=400.........\n[CV 2/5; 71/72] END criterion=entropy, max_depth=15, n_estimators=400; total time=   1.4s\n[CV 3/5; 71/72] START criterion=entropy, max_depth=15, n_estimators=400.........\n[CV 3/5; 71/72] END criterion=entropy, max_depth=15, n_estimators=400; total time=   1.4s\n[CV 4/5; 71/72] START criterion=entropy, max_depth=15, n_estimators=400.........\n[CV 4/5; 71/72] END criterion=entropy, max_depth=15, n_estimators=400; total time=   1.4s\n[CV 5/5; 71/72] START criterion=entropy, max_depth=15, n_estimators=400.........\n[CV 5/5; 71/72] END criterion=entropy, max_depth=15, n_estimators=400; total time=   1.4s\n[CV 1/5; 72/72] START criterion=entropy, max_depth=15, n_estimators=500.........\n[CV 1/5; 72/72] END criterion=entropy, max_depth=15, n_estimators=500; total time=   1.8s\n[CV 2/5; 72/72] START criterion=entropy, max_depth=15, n_estimators=500.........\n[CV 2/5; 72/72] END criterion=entropy, max_depth=15, n_estimators=500; total time=   1.8s\n[CV 3/5; 72/72] START criterion=entropy, max_depth=15, n_estimators=500.........\n[CV 3/5; 72/72] END criterion=entropy, max_depth=15, n_estimators=500; total time=   1.8s\n[CV 4/5; 72/72] START criterion=entropy, max_depth=15, n_estimators=500.........\n[CV 4/5; 72/72] END criterion=entropy, max_depth=15, n_estimators=500; total time=   1.8s\n[CV 5/5; 72/72] START criterion=entropy, max_depth=15, n_estimators=500.........\n[CV 5/5; 72/72] END criterion=entropy, max_depth=15, n_estimators=500; total time=   1.8s\nBest score: 0.8869999999999999\nBest parameters set:\n\tcriterion: entropy\n\tmax_depth: 11\n\tn_estimators: 300\n","output_type":"stream"}]},{"cell_type":"code","source":"# rf_randomized_search.py\nimport numpy as np\nimport pandas as pd\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn import model_selection\n\nif __name__ == \"__main__\":\n    # read the training data\n    df = pd.read_csv(\"/kaggle/input/aaamlp/mobile_train.csv\")\n    # features are all columns without price_range\n    # note that there is no id column in this dataset\n    # here we have training features\n    X = df.drop(\"price_range\", axis=1).values\n    # and the targets\n    y = df.price_range.values\n    # define the model here\n    # i am using random forest with n_jobs=-1\n    # n_jobs=-1 => use all cores\n    classifier = ensemble.RandomForestClassifier(n_jobs=-1)\n    # define a grid of parameters\n    # this can be a dictionary or a list of\n    # dictionaries\n    param_grid = {\n        \"n_estimators\": np.arange(100, 1500, 100),\n        \"max_depth\": np.arange(1, 31),\n        \"criterion\": [\"gini\", \"entropy\"]\n    }\n    # initialize grid search\n    # estimator is the model that we have defined\n    # param_grid is the grid of parameters\n    # we use accuracy as our metric. you can define your own\n    # higher value of verbose implies a lot of details are printed\n    # cv=5 means that we are using 5 fold cv (not stratified)\n    model = model_selection.RandomizedSearchCV(\n    estimator=classifier,\n    param_distributions=param_grid,\n    scoring=\"accuracy\",\n    verbose=10,\n    n_jobs=1,\n    cv=5\n    )\n    # fit the model and extract best score\n    model.fit(X, y)\n    print(f\"Best score: {model.best_score_}\")\n    print(\"Best parameters set:\")\n    best_parameters = model.best_estimator_.get_params()\n    for param_name in sorted(param_grid.keys()):\n        print(f\"\\t{param_name}: {best_parameters[param_name]}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-21T06:39:53.549070Z","iopub.execute_input":"2021-05-21T06:39:53.549461Z","iopub.status.idle":"2021-05-21T06:41:47.118122Z","shell.execute_reply.started":"2021-05-21T06:39:53.549430Z","shell.execute_reply":"2021-05-21T06:41:47.117163Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 10 candidates, totalling 50 fits\n[CV 1/5; 1/10] START criterion=entropy, max_depth=5, n_estimators=300...........\n[CV 1/5; 1/10] END criterion=entropy, max_depth=5, n_estimators=300; total time=   0.9s\n[CV 2/5; 1/10] START criterion=entropy, max_depth=5, n_estimators=300...........\n[CV 2/5; 1/10] END criterion=entropy, max_depth=5, n_estimators=300; total time=   1.0s\n[CV 3/5; 1/10] START criterion=entropy, max_depth=5, n_estimators=300...........\n[CV 3/5; 1/10] END criterion=entropy, max_depth=5, n_estimators=300; total time=   1.0s\n[CV 4/5; 1/10] START criterion=entropy, max_depth=5, n_estimators=300...........\n[CV 4/5; 1/10] END criterion=entropy, max_depth=5, n_estimators=300; total time=   0.9s\n[CV 5/5; 1/10] START criterion=entropy, max_depth=5, n_estimators=300...........\n[CV 5/5; 1/10] END criterion=entropy, max_depth=5, n_estimators=300; total time=   1.0s\n[CV 1/5; 2/10] START criterion=gini, max_depth=13, n_estimators=400.............\n[CV 1/5; 2/10] END criterion=gini, max_depth=13, n_estimators=400; total time=   1.2s\n[CV 2/5; 2/10] START criterion=gini, max_depth=13, n_estimators=400.............\n[CV 2/5; 2/10] END criterion=gini, max_depth=13, n_estimators=400; total time=   1.2s\n[CV 3/5; 2/10] START criterion=gini, max_depth=13, n_estimators=400.............\n[CV 3/5; 2/10] END criterion=gini, max_depth=13, n_estimators=400; total time=   1.2s\n[CV 4/5; 2/10] START criterion=gini, max_depth=13, n_estimators=400.............\n[CV 4/5; 2/10] END criterion=gini, max_depth=13, n_estimators=400; total time=   1.1s\n[CV 5/5; 2/10] START criterion=gini, max_depth=13, n_estimators=400.............\n[CV 5/5; 2/10] END criterion=gini, max_depth=13, n_estimators=400; total time=   1.2s\n[CV 1/5; 3/10] START criterion=gini, max_depth=8, n_estimators=1000.............\n[CV 1/5; 3/10] END criterion=gini, max_depth=8, n_estimators=1000; total time=   2.5s\n[CV 2/5; 3/10] START criterion=gini, max_depth=8, n_estimators=1000.............\n[CV 2/5; 3/10] END criterion=gini, max_depth=8, n_estimators=1000; total time=   2.5s\n[CV 3/5; 3/10] START criterion=gini, max_depth=8, n_estimators=1000.............\n[CV 3/5; 3/10] END criterion=gini, max_depth=8, n_estimators=1000; total time=   2.4s\n[CV 4/5; 3/10] START criterion=gini, max_depth=8, n_estimators=1000.............\n[CV 4/5; 3/10] END criterion=gini, max_depth=8, n_estimators=1000; total time=   2.5s\n[CV 5/5; 3/10] START criterion=gini, max_depth=8, n_estimators=1000.............\n[CV 5/5; 3/10] END criterion=gini, max_depth=8, n_estimators=1000; total time=   2.4s\n[CV 1/5; 4/10] START criterion=gini, max_depth=19, n_estimators=100.............\n[CV 1/5; 4/10] END criterion=gini, max_depth=19, n_estimators=100; total time=   0.4s\n[CV 2/5; 4/10] START criterion=gini, max_depth=19, n_estimators=100.............\n[CV 2/5; 4/10] END criterion=gini, max_depth=19, n_estimators=100; total time=   0.4s\n[CV 3/5; 4/10] START criterion=gini, max_depth=19, n_estimators=100.............\n[CV 3/5; 4/10] END criterion=gini, max_depth=19, n_estimators=100; total time=   0.4s\n[CV 4/5; 4/10] START criterion=gini, max_depth=19, n_estimators=100.............\n[CV 4/5; 4/10] END criterion=gini, max_depth=19, n_estimators=100; total time=   0.4s\n[CV 5/5; 4/10] START criterion=gini, max_depth=19, n_estimators=100.............\n[CV 5/5; 4/10] END criterion=gini, max_depth=19, n_estimators=100; total time=   0.4s\n[CV 1/5; 5/10] START criterion=entropy, max_depth=21, n_estimators=1400.........\n[CV 1/5; 5/10] END criterion=entropy, max_depth=21, n_estimators=1400; total time=   4.5s\n[CV 2/5; 5/10] START criterion=entropy, max_depth=21, n_estimators=1400.........\n[CV 2/5; 5/10] END criterion=entropy, max_depth=21, n_estimators=1400; total time=   4.7s\n[CV 3/5; 5/10] START criterion=entropy, max_depth=21, n_estimators=1400.........\n[CV 3/5; 5/10] END criterion=entropy, max_depth=21, n_estimators=1400; total time=   4.6s\n[CV 4/5; 5/10] START criterion=entropy, max_depth=21, n_estimators=1400.........\n[CV 4/5; 5/10] END criterion=entropy, max_depth=21, n_estimators=1400; total time=   4.6s\n[CV 5/5; 5/10] START criterion=entropy, max_depth=21, n_estimators=1400.........\n[CV 5/5; 5/10] END criterion=entropy, max_depth=21, n_estimators=1400; total time=   4.7s\n[CV 1/5; 6/10] START criterion=gini, max_depth=25, n_estimators=500.............\n[CV 1/5; 6/10] END criterion=gini, max_depth=25, n_estimators=500; total time=   1.4s\n[CV 2/5; 6/10] START criterion=gini, max_depth=25, n_estimators=500.............\n[CV 2/5; 6/10] END criterion=gini, max_depth=25, n_estimators=500; total time=   1.4s\n[CV 3/5; 6/10] START criterion=gini, max_depth=25, n_estimators=500.............\n[CV 3/5; 6/10] END criterion=gini, max_depth=25, n_estimators=500; total time=   1.4s\n[CV 4/5; 6/10] START criterion=gini, max_depth=25, n_estimators=500.............\n[CV 4/5; 6/10] END criterion=gini, max_depth=25, n_estimators=500; total time=   1.4s\n[CV 5/5; 6/10] START criterion=gini, max_depth=25, n_estimators=500.............\n[CV 5/5; 6/10] END criterion=gini, max_depth=25, n_estimators=500; total time=   1.4s\n[CV 1/5; 7/10] START criterion=entropy, max_depth=8, n_estimators=1400..........\n[CV 1/5; 7/10] END criterion=entropy, max_depth=8, n_estimators=1400; total time=   4.1s\n[CV 2/5; 7/10] START criterion=entropy, max_depth=8, n_estimators=1400..........\n[CV 2/5; 7/10] END criterion=entropy, max_depth=8, n_estimators=1400; total time=   4.2s\n[CV 3/5; 7/10] START criterion=entropy, max_depth=8, n_estimators=1400..........\n[CV 3/5; 7/10] END criterion=entropy, max_depth=8, n_estimators=1400; total time=   4.5s\n[CV 4/5; 7/10] START criterion=entropy, max_depth=8, n_estimators=1400..........\n[CV 4/5; 7/10] END criterion=entropy, max_depth=8, n_estimators=1400; total time=   4.1s\n[CV 5/5; 7/10] START criterion=entropy, max_depth=8, n_estimators=1400..........\n[CV 5/5; 7/10] END criterion=entropy, max_depth=8, n_estimators=1400; total time=   3.9s\n[CV 1/5; 8/10] START criterion=gini, max_depth=9, n_estimators=1000.............\n[CV 1/5; 8/10] END criterion=gini, max_depth=9, n_estimators=1000; total time=   2.5s\n[CV 2/5; 8/10] START criterion=gini, max_depth=9, n_estimators=1000.............\n[CV 2/5; 8/10] END criterion=gini, max_depth=9, n_estimators=1000; total time=   2.5s\n[CV 3/5; 8/10] START criterion=gini, max_depth=9, n_estimators=1000.............\n[CV 3/5; 8/10] END criterion=gini, max_depth=9, n_estimators=1000; total time=   2.4s\n[CV 4/5; 8/10] START criterion=gini, max_depth=9, n_estimators=1000.............\n[CV 4/5; 8/10] END criterion=gini, max_depth=9, n_estimators=1000; total time=   2.5s\n[CV 5/5; 8/10] START criterion=gini, max_depth=9, n_estimators=1000.............\n[CV 5/5; 8/10] END criterion=gini, max_depth=9, n_estimators=1000; total time=   2.5s\n[CV 1/5; 9/10] START criterion=entropy, max_depth=22, n_estimators=800..........\n[CV 1/5; 9/10] END criterion=entropy, max_depth=22, n_estimators=800; total time=   2.8s\n[CV 2/5; 9/10] START criterion=entropy, max_depth=22, n_estimators=800..........\n[CV 2/5; 9/10] END criterion=entropy, max_depth=22, n_estimators=800; total time=   2.7s\n[CV 3/5; 9/10] START criterion=entropy, max_depth=22, n_estimators=800..........\n[CV 3/5; 9/10] END criterion=entropy, max_depth=22, n_estimators=800; total time=   2.7s\n[CV 4/5; 9/10] START criterion=entropy, max_depth=22, n_estimators=800..........\n[CV 4/5; 9/10] END criterion=entropy, max_depth=22, n_estimators=800; total time=   2.9s\n[CV 5/5; 9/10] START criterion=entropy, max_depth=22, n_estimators=800..........\n[CV 5/5; 9/10] END criterion=entropy, max_depth=22, n_estimators=800; total time=   2.8s\n[CV 1/5; 10/10] START criterion=gini, max_depth=2, n_estimators=700.............\n[CV 1/5; 10/10] END criterion=gini, max_depth=2, n_estimators=700; total time=   1.4s\n[CV 2/5; 10/10] START criterion=gini, max_depth=2, n_estimators=700.............\n[CV 2/5; 10/10] END criterion=gini, max_depth=2, n_estimators=700; total time=   1.4s\n[CV 3/5; 10/10] START criterion=gini, max_depth=2, n_estimators=700.............\n[CV 3/5; 10/10] END criterion=gini, max_depth=2, n_estimators=700; total time=   1.4s\n[CV 4/5; 10/10] START criterion=gini, max_depth=2, n_estimators=700.............\n[CV 4/5; 10/10] END criterion=gini, max_depth=2, n_estimators=700; total time=   1.4s\n[CV 5/5; 10/10] START criterion=gini, max_depth=2, n_estimators=700.............\n[CV 5/5; 10/10] END criterion=gini, max_depth=2, n_estimators=700; total time=   1.4s\nBest score: 0.8870000000000001\nBest parameters set:\n\tcriterion: entropy\n\tmax_depth: 22\n\tn_estimators: 800\n","output_type":"stream"}]},{"cell_type":"code","source":"# rf_gp_minimize.py\nimport numpy as np\nimport pandas as pd\nfrom functools import partial\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom skopt import gp_minimize\nfrom skopt import space\n\ndef optimize(params, param_names, x, y):\n    \"\"\"\n    The main optimization function.\n    This function takes all the arguments from the search space\n    and training features and targets. It then initializes\n    the models by setting the chosen parameters and runs\n    cross-validation and returns a negative accuracy score\n    :param params: list of params from gp_minimize\n    :param param_names: list of param names. order is important!\n    :param x: training data\n    :param y: labels/targets\n    :return: negative accuracy after 5 folds\n    \"\"\"\n    # convert params to dictionary\n    params = dict(zip(param_names, params))\n    # initialize model with current parameters\n    model = ensemble.RandomForestClassifier(**params)\n    # initialize stratified k-fold\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    # initialize accuracy list\n    accuracies = []\n    # loop over all folds\n    for idx in kf.split(X=x, y=y):\n        train_idx, test_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        xtest = x[test_idx]\n        ytest = y[test_idx]\n        # fit model for current fold\n        model.fit(xtrain, ytrain)\n        #create predictions\n        preds = model.predict(xtest)\n        # calculate and append accuracy\n        fold_accuracy = metrics.accuracy_score(\n        ytest,\n        preds\n        )\n        accuracies.append(fold_accuracy)\n    # return negative accuracy\n    return -1 * np.mean(accuracies)\n\nif __name__ == \"__main__\":\n    # read the training data\n    df = pd.read_csv(\"../input/aaamlp/mobile_train.csv\")\n    #features are all columns without price_range\n    # note that there is no id column in this dataset\n    # here we have training features\n    X = df.drop(\"price_range\", axis=1).values\n    # and the targets\n    y = df.price_range.values\n    # define a parameter space\n    param_space = [\n        # max_depth is an integer between 3 and 10\n        space.Integer(3, 15, name=\"max_depth\"),\n        # n_estimators is an integer between 50 and 1500\n        space.Integer(100, 1500, name=\"n_estimators\"),\n        # criterion is a category. here we define list of categories\n        space.Categorical([\"gini\", \"entropy\"], name=\"criterion\"),\n        # you can also have Real numbered space and define a\n        # distribution you want to pick it from\n        space.Real(0.01, 1, prior=\"uniform\", name=\"max_features\")\n    ]\n    # make a list of param names\n    # this has to be same order as the search space\n    # inside the main function\n    param_names = [\n        \"max_depth\",\n        \"n_estimators\",\n        \"criterion\",\n        \"max_features\"\n        ]\n    # by using functools partial, i am creating a\n    # new function which has same parameters as the\n    # optimize function except for the fact that\n    # only one param, i.e. the \"params\" parameter is\n    # required. this is how gp_minimize expects the\n    # optimization function to be. you can get rid of this\n    # by reading data inside the optimize function or by\n    # defining the optimize function here.\n    optimization_function = partial(\n        optimize,\n        param_names=param_names,\n        x=X,\n        y=y\n    )\n    # now we call gp_minimize from scikit-optimize\n    # gp_minimize uses bayesian optimization for\n    # minimization of the optimization function.\n    # we need a space of parameters, the function itself,\n    # the number of calls/iterations we want to have\n    result = gp_minimize(\n        optimization_function,\n        dimensions=param_space,\n        n_calls=15,\n        n_random_starts=10,\n        verbose=10\n    )\n    # create best params dict and print it\n    best_params = dict(\n    zip(\n        param_names,\n        result.x\n    )\n    )\n    print(best_params)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T07:26:32.571207Z","iopub.execute_input":"2021-05-21T07:26:32.571978Z","iopub.status.idle":"2021-05-21T07:37:01.653880Z","shell.execute_reply.started":"2021-05-21T07:26:32.571925Z","shell.execute_reply":"2021-05-21T07:37:01.652906Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Iteration No: 1 started. Evaluating function at random point.\nIteration No: 1 ended. Evaluation done at random point.\nTime taken: 7.2940\nFunction value obtained: -0.8915\nCurrent minimum: -0.8915\nIteration No: 2 started. Evaluating function at random point.\nIteration No: 2 ended. Evaluation done at random point.\nTime taken: 7.4797\nFunction value obtained: -0.8545\nCurrent minimum: -0.8915\nIteration No: 3 started. Evaluating function at random point.\nIteration No: 3 ended. Evaluation done at random point.\nTime taken: 29.0873\nFunction value obtained: -0.8320\nCurrent minimum: -0.8915\nIteration No: 4 started. Evaluating function at random point.\nIteration No: 4 ended. Evaluation done at random point.\nTime taken: 45.7923\nFunction value obtained: -0.8880\nCurrent minimum: -0.8915\nIteration No: 5 started. Evaluating function at random point.\nIteration No: 5 ended. Evaluation done at random point.\nTime taken: 24.7324\nFunction value obtained: -0.8275\nCurrent minimum: -0.8915\nIteration No: 6 started. Evaluating function at random point.\nIteration No: 6 ended. Evaluation done at random point.\nTime taken: 52.3570\nFunction value obtained: -0.8795\nCurrent minimum: -0.8915\nIteration No: 7 started. Evaluating function at random point.\nIteration No: 7 ended. Evaluation done at random point.\nTime taken: 82.1752\nFunction value obtained: -0.9080\nCurrent minimum: -0.9080\nIteration No: 8 started. Evaluating function at random point.\nIteration No: 8 ended. Evaluation done at random point.\nTime taken: 61.9003\nFunction value obtained: -0.8960\nCurrent minimum: -0.9080\nIteration No: 9 started. Evaluating function at random point.\nIteration No: 9 ended. Evaluation done at random point.\nTime taken: 13.6534\nFunction value obtained: -0.8160\nCurrent minimum: -0.9080\nIteration No: 10 started. Evaluating function at random point.\nIteration No: 10 ended. Evaluation done at random point.\nTime taken: 8.5433\nFunction value obtained: -0.8585\nCurrent minimum: -0.9080\nIteration No: 11 started. Searching for the next optimal point.\nIteration No: 11 ended. Search finished for the next optimal point.\nTime taken: 19.5949\nFunction value obtained: -0.7790\nCurrent minimum: -0.9080\nIteration No: 12 started. Searching for the next optimal point.\nIteration No: 12 ended. Search finished for the next optimal point.\nTime taken: 58.1126\nFunction value obtained: -0.8945\nCurrent minimum: -0.9080\nIteration No: 13 started. Searching for the next optimal point.\nIteration No: 13 ended. Search finished for the next optimal point.\nTime taken: 54.2999\nFunction value obtained: -0.8305\nCurrent minimum: -0.9080\nIteration No: 14 started. Searching for the next optimal point.\nIteration No: 14 ended. Search finished for the next optimal point.\nTime taken: 95.5083\nFunction value obtained: -0.9025\nCurrent minimum: -0.9080\nIteration No: 15 started. Searching for the next optimal point.\nIteration No: 15 ended. Search finished for the next optimal point.\nTime taken: 68.4733\nFunction value obtained: -0.9060\nCurrent minimum: -0.9080\n{'max_depth': 10, 'n_estimators': 1330, 'criterion': 'entropy', 'max_features': 0.9836382162032888}\n","output_type":"stream"}]},{"cell_type":"code","source":"from skopt.plots import plot_convergence\nplot_convergence(result)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T07:55:09.828614Z","iopub.execute_input":"2021-05-21T07:55:09.829051Z","iopub.status.idle":"2021-05-21T07:55:10.280413Z","shell.execute_reply.started":"2021-05-21T07:55:09.829017Z","shell.execute_reply":"2021-05-21T07:55:10.279083Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:title={'center':'Convergence plot'}, xlabel='Number of calls $n$', ylabel='$\\\\min f(x)$ after $n$ calls'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"# rf_hyperopt.py\nimport numpy as np\nimport pandas as pd\nfrom functools import partial\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\n\ndef optimize(params, x, y):\n    \"\"\"\n    The main optimization function.\n    This function takes all the arguments from the search space\n    and training features and targets. It then initializes\n    the models by setting the chosen parameters and runs\n    cross-validation and returns a negative accuracy score\n    :param params: list of params from gp_minimize\n    :param param_names: list of param names. order is important!\n    :param x: training data\n    :param y: labels/targets\n    :return: negative accuracy after 5 folds\n    \"\"\"\n    \n    # initialize model with current parameters\n    model = ensemble.RandomForestClassifier(**params)\n    # initialize stratified k-fold\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    # initialize accuracy list\n    accuracies = []\n    # loop over all folds\n    for idx in kf.split(X=x, y=y):\n        train_idx, test_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        xtest = x[test_idx]\n        ytest = y[test_idx]\n        # fit model for current fold\n        model.fit(xtrain, ytrain)\n        #create predictions\n        preds = model.predict(xtest)\n        # calculate and append accuracy\n        fold_accuracy = metrics.accuracy_score(\n        ytest,\n        preds\n        )\n        accuracies.append(fold_accuracy)\n    # return negative accuracy\n    return -1 * np.mean(accuracies)\n\nif __name__ == \"__main__\":\n    # read the training data\n    df = pd.read_csv(\"../input/aaamlp/mobile_train.csv\")\n\n    # features are all columns without price_range\n    # note that there is no id column in this dataset\n    # here we have training features\n    X = df.drop(\"price_range\", axis=1).values\n    # and the targets\n    y = df.price_range.values\n    # define a parameter space\n    # now we use hyperopt\n    param_space = {\n        # quniform gives round(uniform(low, high) / q) * q\n        # we want int values for depth and estimators\n        \"max_depth\": scope.int(hp.quniform(\"max_depth\", 1, 15, 1)),\n        \"n_estimators\": scope.int(\n        hp.quniform(\"n_estimators\", 100, 1500, 1)\n        ),\n        # choice chooses from a list of values\n        \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n        # uniform chooses a value between two values\n        \"max_features\": hp.uniform(\"max_features\", 0, 1)\n    }\n    # partial function\n    optimization_function = partial(\n        optimize,\n        x=X,\n        y=y\n    )\n    # initialize trials to keep logging information\n    trials = Trials()\n    # run hyperopt\n    hopt = fmin(\n        fn=optimization_function,\n        space=param_space,\n        algo=tpe.suggest,\n        max_evals=15,\n        trials=trials\n    )\n    print(hopt)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:05:41.876609Z","iopub.execute_input":"2021-05-21T08:05:41.877055Z","iopub.status.idle":"2021-05-21T08:11:37.799948Z","shell.execute_reply.started":"2021-05-21T08:05:41.877017Z","shell.execute_reply":"2021-05-21T08:11:37.798742Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"100%|| 15/15 [05:55<00:00, 23.73s/trial, best loss: -0.909]            \n{'criterion': 1, 'max_depth': 14.0, 'max_features': 0.7716461340431485, 'n_estimators': 558.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}